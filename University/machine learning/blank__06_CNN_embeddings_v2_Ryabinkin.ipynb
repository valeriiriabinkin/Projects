{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XtFQP3RNll3c",
    "outputId": "ef7f0234-22f1-4de0-ab4b-19ec0b0e190d"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nabr9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "nltk.download('punkt')\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\nabr9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\nabr9\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "from tqdm.notebook import tqdm\n",
    "import sys\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"stopwords\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1aszp7KbOstY",
    "outputId": "70fcaf74-660f-45b9-cb65-352a603b23c2"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9D6VU-qbO68-",
    "outputId": "76b09aa4-8f1e-4db6-e071-0c6f31a07158"
   },
   "outputs": [],
   "source": [
    "cd drive/MyDrive/datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tx75RigN8xIJ"
   },
   "source": [
    "## 1. Представление и предобработка текстовых данных в виде последовательностей"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LScKIAey9dAM"
   },
   "source": [
    "1.1 Представьте первое предложение из строки `text` как последовательность из индексов слов, входящих в это предложение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "phEw721T9SYW"
   },
   "outputs": [],
   "source": [
    "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sVr5jv_1eApB",
    "outputId": "387d72ea-56f1-4c0b-c21e-5e2d2ccd310e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_list = word_tokenize(re.sub('[^a-zA-z0-9]', ' ', sent_tokenize(text)[0])); [text_list.index(i) for i in text_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pSFQCPtD9x5J"
   },
   "source": [
    "1.2 Представьте первое предложение из строки `text` как последовательность векторов, соответствующих индексам слов. Для представления индекса в виде вектора используйте унитарное кодирование. В результате должен получиться двумерный тензор размера `количество слов в предложении` x `количество уникальных слов`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "RZS4XLV0-buf"
   },
   "outputs": [],
   "source": [
    "text = 'Select your preferences and run the install command. Stable represents the most currently tested and supported version of PyTorch. Note that LibTorch is only available for C++'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "swSFaZ4YhHVU",
    "outputId": "18eeefcd-6574-4885-94a8-91948768abc0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "         0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "onehot = torch.zeros([len(text_list), len(np.unique(word_tokenize(re.sub('[^a-zA-z0-9]', ' ', text))))]); onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "I6iXCP-plmfG"
   },
   "outputs": [],
   "source": [
    "uniword = np.unique(word_tokenize(re.sub('[^a-zA-z0-9]', ' ', text))).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VYAStEqlkR_-",
    "outputId": "38f99f81-401d-427c-9a8a-451bb7140ee8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 1.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         1., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 1., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
      "         0., 0., 0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(text_list)):\n",
    "  if text_list[i] in uniword:\n",
    "    indexx = uniword.index(text_list[i])\n",
    "    onehot[i][indexx] = 1\n",
    "print(onehot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9ZvQKHYA-mJN"
   },
   "source": [
    "1.3 Решите задачу 1.2, используя модуль `nn.Embedding`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "KOMX6Hvmnyj8"
   },
   "outputs": [],
   "source": [
    "embedding = nn.Embedding(len(np.unique(word_tokenize(re.sub('[^a-zA-z0-9]', ' ', text)))), len(text_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "wjicvPJmpCfv"
   },
   "outputs": [],
   "source": [
    "for_indexes = torch.zeros(8)\n",
    "for i in range(len(text_list)):\n",
    "  if text_list[i] in uniword:\n",
    "    for_indexes[i] = uniword.index(text_list[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4, 24, 16,  6, 18, 22, 11,  8])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for_indexes.long()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 328
    },
    "id": "ixtw3Ym7qPmv",
    "outputId": "c694a0ea-56df-4021-f068-0c7be1d2deea"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2179, -0.5288, -0.2822,  2.8217,  3.4018,  1.2108, -0.2459,  2.3452],\n",
       "        [ 1.4329, -1.6946,  0.4885, -1.5158,  1.7172, -1.2646, -0.8125, -0.1755],\n",
       "        [ 0.7794,  2.8786,  0.1736, -0.9336,  1.9461,  0.3923, -1.0480,  1.6091],\n",
       "        [ 0.8953, -0.8241,  0.0416,  0.7857,  0.3699,  1.9490,  0.3454,  0.7366],\n",
       "        [ 1.0175,  1.3943,  2.0876, -0.5446, -0.9292, -1.1559,  0.5718,  0.0385],\n",
       "        [-0.1371, -1.6952, -0.4434,  0.1350,  0.7347, -0.1113,  0.7076, -0.1546],\n",
       "        [ 0.3355,  1.5924, -1.2040,  1.3352,  0.5897, -0.2142,  1.0840, -0.2622],\n",
       "        [ 0.8615, -0.4213, -0.3978, -0.4247,  1.6595, -1.6349, -0.4552,  0.0810]],\n",
       "       grad_fn=<EmbeddingBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding(for_indexes.long())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TXjM7qEUNFY_"
   },
   "source": [
    "## 2. Классификация фамилий по национальности (ConvNet)\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/owHew8hzPc7X9Q?w=1\n",
    "\n",
    "2.1 Считать файл `surnames/surnames.csv`. \n",
    "\n",
    "2.2 Закодировать национальности числами, начиная с 0.\n",
    "\n",
    "2.3 Разбить датасет на обучающую и тестовую выборку\n",
    "\n",
    "2.4 Реализовать класс `Vocab` (токен = __символ__)\n",
    "  * добавьте в словарь специальный токен `<PAD>` с индексом 0\n",
    "  * при создании словаря сохраните длину самой длинной последовательности из набора данных в виде атрибута `max_seq_len`\n",
    "\n",
    "2.5 Реализовать класс `SurnamesDataset`\n",
    "  * метод `__getitem__` возвращает пару: <последовательность индексов токенов (см. 1.1 ), номер класса> \n",
    "  * длина каждой такой последовательности должна быть одинаковой и равной `vocab.max_seq_len`. Чтобы добиться этого, дополните последовательность справа индексом токена `<PAD>` до нужной длины\n",
    "\n",
    "2.6. Обучить классификатор.\n",
    "  \n",
    "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding`. Рассмотрите два варианта: \n",
    "    - когда токен представляется в виде унитарного вектора и модуль `nn.Embedding` не обучается\n",
    "    - когда токен представляется в виде вектора небольшой размерности (меньше, чем размер словаря) и модуль `nn.Embedding` обучается\n",
    "\n",
    "  * Используйте одномерные свертки и пулинг (`nn.Conv1d`, `nn.MaxPool1d`)\n",
    "    - обратите внимание, что `nn.Conv1d` ожидает на вход трехмерный тензор размерности `(batch, embedding_dim, seq_len)`\n",
    "\n",
    "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: прогнать несколько фамилий студентов группы через модели и проверить результат. Для каждой фамилии выводить 3 наиболее вероятных предсказания."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('surnames.csv')\n",
    "data['nationality'] = pd.factorize(data['nationality'])[0]\n",
    "result_df = data.drop_duplicates(subset=['surname'], keep='first')\n",
    "y = result_df['nationality']\n",
    "X = result_df['surname']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "ZGfJX2NP1sw4"
   },
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "  def __init__(self, data):\n",
    "        letters = ['<PAD>']\n",
    "        for i in data:\n",
    "            for j in i:\n",
    "                if j not in letters:\n",
    "                    letters.append(j)\n",
    "        self.idx_to_token = {idx: val for idx, val in enumerate(letters)}\n",
    "        self.token_to_idx = {val: idx for idx, val in self.idx_to_token.items()}\n",
    "        self.vocab_len = len(self.idx_to_token)\n",
    "        self.max_seq_len = max([len(i) for i in data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vocab = Vocab(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "GHjCRqQg1sw5"
   },
   "outputs": [],
   "source": [
    "class SurnamesDataset(Dataset):\n",
    "    def __init__(self, X, y, vocab: Vocab):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.vocab = vocab\n",
    "    \n",
    "    def vectorize(self, surname):\n",
    "        abc = []\n",
    "        for i in surname:\n",
    "            if i in list(self.vocab.idx_to_token.values()):\n",
    "                get_index = list(self.vocab.idx_to_token.values()).index(i)\n",
    "                abc.append(get_index)\n",
    "        while len(abc)<self.vocab.max_seq_len:\n",
    "            abc.append(0)\n",
    "        return torch.tensor(abc, dtype=torch.int64)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.vectorize(self.X.iloc[index]), self.y.iloc[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class Vocab:\n",
    "#   def __init__(self, data: list):\n",
    "#     unique_ch = [\"<PAD>\"] + list(set(\"\".join(data)))\n",
    "#     ch_ind = list(np.arange(len(unique_ch)))\n",
    "#     self.idx_to_token = dict(zip(ch_ind, unique_ch))\n",
    "#     self.token_to_idx = dict(zip(unique_ch, ch_ind))\n",
    "#     self.vocab_len = len(unique_ch)\n",
    "#     self.max_seq_len = max(list(map(len, data)))\n",
    "\n",
    "\n",
    "# class SurnamesDataset(Dataset):\n",
    "#   def __init__(self, X, y, vocab: Vocab):\n",
    "#     self.X = X\n",
    "#     self.y = y\n",
    "#     self.vocab = vocab\n",
    "\n",
    "#   def get_idx(self, surname):\n",
    "#     idx = [0] * self.vocab.max_seq_len\n",
    "#     for i, c in enumerate(surname):\n",
    "#       idx[i] = self.vocab.token_to_idx[c]\n",
    "    \n",
    "#     return torch.tensor(idx, dtype=torch.int64)\n",
    "\n",
    "\n",
    "#   def __len__(self):\n",
    "#     return len(self.X)\n",
    "\n",
    "#   def __getitem__(self, idx):\n",
    "#     return self.get_idx(self.X.iloc[idx]), self.y.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(X.tolist())\n",
    "surname_train = SurnamesDataset(X_train, y_train, vocab)\n",
    "surname_test = SurnamesDataset(X_test, y_test, vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = SurnamesDataset(X_test, y_test, Vocab(X))\n",
    "train_ds = SurnamesDataset(X_train, y_train, Vocab(X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "train_iter = DataLoader(train_ds, batch_size, shuffle=True)\n",
    "test_iter = DataLoader(test_ds, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fam_model(nn.Module):\n",
    "    def __init__(self, input_size, vocab_size, num_channels, output_size):\n",
    "        super(fam_model, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        self.embedding = nn.Embedding(self.vocab_size, input_size)\n",
    "        self.lr1 = nn.Conv1d(in_channels=input_size, out_channels=num_channels, kernel_size=3)\n",
    "        self.lr2 = nn.Conv1d(in_channels=num_channels, out_channels=num_channels, kernel_size=3)\n",
    "        self.lr3 = nn.Conv1d(in_channels=num_channels, out_channels=num_channels, kernel_size=3)\n",
    "        self.lr4 = nn.Conv1d(in_channels=num_channels, out_channels=num_channels, kernel_size=3)\n",
    "        self.relu = nn.ELU()\n",
    "        self.norm = nn.BatchNorm1d(num_channels)\n",
    "        self.maxpool = nn.MaxPool1d(kernel_size = 3)\n",
    "        self.lr5 = nn.Linear(num_channels, self.output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.lr1(out)\n",
    "        out = self.norm(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.lr2(out)\n",
    "        out = self.norm(out)\n",
    "        out = self.relu(out).squeeze(dim=2)\n",
    "        out = self.lr5(out)\n",
    "        return torch.nn.functional.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Модель 1\n",
    "\n",
    "class sur_model(nn.Module):\n",
    "    def __init__(self, input_size, vocab_size, num_channels, output_size):\n",
    "        super(sur_model, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seqnet = nn.Sequential(nn.Embedding(self.vocab_size, input_size),\n",
    "            nn.Conv1d(input_size, 64, 3),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool1d(2),                        \n",
    "            nn.Conv1d(64, 256, 2),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool1d(2),                        \n",
    "            nn.Conv1d(256, 512, 2),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(512, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "        self.fc = nn.Linear(64, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "      out = self.seqnet(x)\n",
    "      out = self.fc(out)\n",
    "      return torch.nn.functional.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = vocab.max_seq_len\n",
    "vocab_size = vocab.vocab_len\n",
    "output_size = len(set(data['nationality']))\n",
    "num_channels = 300\n",
    "model = sur_model(input_size, vocab_size, num_channels, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dl, optimizer, total):\n",
    "    #set model in train() mode:\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_samples = 0.0\n",
    "    correct_samples = 0.0\n",
    "    \n",
    "    for i, (inputs, targets) in tqdm(enumerate(train_dl), total= total, desc='Training minibatch loop '):\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward and optimize        \n",
    "        # zero grad before new step        \n",
    "        optimizer.zero_grad()                        \n",
    "        loss.backward()\n",
    "        optimizer.step()   \n",
    "\n",
    "        # calculating the total_loss for checking\n",
    "        total_loss += loss           \n",
    "        \n",
    "        # PREDICTIONS \n",
    "        total_samples += targets.shape[0]   \n",
    "        _, predictions_indices = torch.max(outputs, 1) # dim=1 - dimension to reduce\n",
    "        correct_samples += torch.sum(predictions_indices==targets)\n",
    "\n",
    "    train_accuracy = float(correct_samples) / total_samples        \n",
    "    \n",
    "    return total_loss, train_accuracy                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING THE MODEL\n",
    "def test(model, device, test_dl, total):\n",
    "    #set model in eval() mode (it skips Dropout etc):\n",
    "    model.eval()\n",
    "    \n",
    "    total_samples = 0.0\n",
    "    correct_samples = 0.0 \n",
    "    \n",
    "    # set the requires_grad flag to false as we are in the test mode\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, targets) in tqdm(enumerate(test_dl), total=total, desc='Testing minibatch loop:'):                      \n",
    "            # apply model to input data\n",
    "            outputs = model(inputs)        \n",
    "                       \n",
    "            #PREDICTIONS\n",
    "            total_samples += targets.shape[0]   \n",
    "            _, predictions_indices = torch.max(outputs, 1) # dim=1 - dimension to reduce\n",
    "            correct_samples += torch.sum(predictions_indices==targets)                    \n",
    "        \n",
    "    test_accuracy = correct_samples / total_samples        \n",
    "    \n",
    "    return test_accuracy              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cdb6cc9ad14f4b078a562648ca28802e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4c1d05876204364b84dcc6ae118d46f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Loss: 176.7764, Train acc: 0.4902, Test acc: 0.5658\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1af85e5a29a743a5be0a6bce0e1b2a0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb7c4f8b817d4dbba9a7895e241d28c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25], Loss: 132.3575, Train acc: 0.6124, Test acc: 0.6218\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72dfd0146e754ab1bea3f671608297dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b2e89f9a9bd94bbab7f3eda58cf5a3d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25], Loss: 107.0665, Train acc: 0.6803, Test acc: 0.6211\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "593b0a284b6e47308a04e000ac32e53c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5312a93ded674f38b9462d3260c792a7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25], Loss: 85.8822, Train acc: 0.7345, Test acc: 0.6285\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17289a771e2840769bac0914bc905b2b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66d9563b28db4bb281488a8c5a44d054",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/25], Loss: 69.5067, Train acc: 0.7788, Test acc: 0.6251\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1079623e22534822af6bff3aa80c7eaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a180fbb6cf044aba42f20a442d20484",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/25], Loss: 54.3872, Train acc: 0.8246, Test acc: 0.6347\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98bbe28475c427b9e218b847f54b128",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e8acee8c9e401e8123660d74d46d89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/25], Loss: 43.4342, Train acc: 0.8545, Test acc: 0.6329\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0594103ad2d6425e8defb54a4d0e6435",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dac4567ef98b4687a8fe4edeed71c88e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/25], Loss: 34.9008, Train acc: 0.8823, Test acc: 0.6358\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f98134dc11954399b0fc16bd9624dff1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f2e18830f1004e74b729d41fd6fb3915",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/25], Loss: 25.7974, Train acc: 0.9150, Test acc: 0.6395\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48654e5c8ae54619862ac80939d8fae4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e8b2b1ee111b49409a50b94c9e9e3f87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/25], Loss: 23.0248, Train acc: 0.9216, Test acc: 0.6074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2af577a14f640efa67771c8ef0d3f89",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66b597cc2a4247feba994ad104aac256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/25], Loss: 21.8774, Train acc: 0.9256, Test acc: 0.6314\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "485245b58bb84952878eacadfaa5aabb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_23756/3480799073.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(model, device, train_dl, optimizer, total)\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m# calculating the total_loss for checking\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     87\u001b[0m                 \u001b[1;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 88\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     89\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     25\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    139\u001b[0m                     \u001b[0mstate_steps\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'step'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 141\u001b[1;33m             F.adam(params_with_grad,\n\u001b[0m\u001b[0;32m    142\u001b[0m                    \u001b[0mgrads\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    143\u001b[0m                    \u001b[0mexp_avgs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\torch\\optim\\_functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[0;32m    108\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 110\u001b[1;33m         \u001b[0mparam\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    111\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    112\u001b[0m def adamw(params: List[Tensor],\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for_losses = []\n",
    "n_total_steps = len(train_iter)\n",
    "\n",
    "\n",
    "for epoch in range(25):\n",
    "    total_loss, train_accuracy = train(model, device, train_iter, optimizer, total = math.ceil(len(train_ds)/64))\n",
    "    test_accuracy = test(model, device, test_iter, total = math.ceil(len(test_ds)/64))\n",
    "    for_losses.append(total_loss.cpu().detach().numpy())\n",
    "    print (f'Epoch [{epoch+1}/{25}], Loss: {total_loss:.4f}, Train acc: {train_accuracy:.4f}, Test acc: {test_accuracy:.4f}')                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sur_model2(nn.Module):\n",
    "    def __init__(self, input_size, vocab_size, num_channels, output_size):\n",
    "        super(sur_model2, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seqnet = nn.Sequential(nn.Embedding(self.vocab_size, input_size),\n",
    "            nn.Conv1d(input_size, 64, 3),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool1d(2),                        \n",
    "            nn.Conv1d(64, 256, 2),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool1d(2),                        \n",
    "            nn.Conv1d(256, 512, 2),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(512, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "        self.fc = nn.Linear(64, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "      out = self.seqnet(x)\n",
    "      out = self.fc(out)\n",
    "      return torch.nn.functional.log_softmax(out, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sur_model2(input_size, vocab_size, num_channels, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подсказал саша волненко\n",
    "layers = next(iter(model.children()))\n",
    "for i, x in enumerate(layers):\n",
    "    if i == 0:\n",
    "        for param in x.parameters():\n",
    "              param.requires_grad = False\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "layers = next(iter(model.children()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Embedding(85, 17)\n",
       "  (1): Conv1d(17, 64, kernel_size=(3,), stride=(1,))\n",
       "  (2): BatchNorm1d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (3): ELU(alpha=1.0)\n",
       "  (4): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv1d(64, 256, kernel_size=(2,), stride=(1,))\n",
       "  (6): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (7): ELU(alpha=1.0)\n",
       "  (8): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (9): Conv1d(256, 512, kernel_size=(2,), stride=(1,))\n",
       "  (10): BatchNorm1d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (11): ELU(alpha=1.0)\n",
       "  (12): MaxPool1d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (13): Flatten(start_dim=1, end_dim=-1)\n",
       "  (14): Linear(in_features=512, out_features=256, bias=True)\n",
       "  (15): ELU(alpha=1.0)\n",
       "  (16): Linear(in_features=256, out_features=128, bias=True)\n",
       "  (17): ELU(alpha=1.0)\n",
       "  (18): Linear(in_features=128, out_features=64, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.NLLLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f24f516e8a674d03b03bd218a9a1dc2d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "777f1f93a95e4155b68689ae3ba5a3d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Loss: 179.2348, Train acc: 0.4867, Test acc: 0.5271\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9182e2885ae048f685f2e09a94fc65ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "619191e21c604308904214a9153125e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25], Loss: 142.5585, Train acc: 0.5833, Test acc: 0.5109\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee06ae5dd3834a5fbda34adccf165699",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d156a6b6c96f48a58f7e023d652ab5b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25], Loss: 121.0653, Train acc: 0.6441, Test acc: 0.6074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3837fc42f21f4a9b8776afac3e18c3c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa91aa37476443b48b7d98e414c72ec6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25], Loss: 101.1188, Train acc: 0.6956, Test acc: 0.6023\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65a9ca00a828441ea5b2d07e7ecdff57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bde2770b3bcc4c378b2061335dbde0c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/25], Loss: 85.5685, Train acc: 0.7355, Test acc: 0.6152\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50af1cba574b4b0cbaeba488fd2ac5cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e600d009a01c418aa9786c0c0099e780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/25], Loss: 70.5043, Train acc: 0.7764, Test acc: 0.6089\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a345f75070c343a99f07d25cb6c2cba5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f0dac0f23364647b6b7a35d07587e56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/25], Loss: 59.1504, Train acc: 0.8116, Test acc: 0.5864\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20c6f8235ba54a9eaba3e69c4f424f41",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "faa898f73640409b82da45489bf7b867",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/25], Loss: 46.6628, Train acc: 0.8500, Test acc: 0.5780\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b93dbe13d14b41259c8503dd5a4c3269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db91f89813e54acc85054fe2fd1e3d45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/25], Loss: 39.4297, Train acc: 0.8696, Test acc: 0.5850\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "029b2691fb9d44c4abb3cb68d8d0ad4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b214d002fa14fb4990326d04238895e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/25], Loss: 33.9134, Train acc: 0.8891, Test acc: 0.6015\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11bdee1a3f174b2e97fbaced8503dfce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f93b25c3a494a77b8574feb9aec3309",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/25], Loss: 28.2801, Train acc: 0.9034, Test acc: 0.6052\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "59a0bedfc5c44ca0aa71a382f630eb13",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcfd34cc68624847bcc4441c17687e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/25], Loss: 23.3315, Train acc: 0.9223, Test acc: 0.5776\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cf688831d2f4f3c976b6d9b38b8790c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8ac6991ca6746fdb3bc436190787c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/25], Loss: 20.6723, Train acc: 0.9292, Test acc: 0.6078\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27f8122d7dae40aca888d0fff5470f7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f8e5e3ab5d04abe9231e0d6f0fcba68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/25], Loss: 16.4721, Train acc: 0.9447, Test acc: 0.6086\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c845bc14e9640d8ab9132548f20b434",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ff25ab6755e449439121a6532ef25f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/25], Loss: 14.5106, Train acc: 0.9526, Test acc: 0.5938\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49448108ebf14cd88641ca4a4deddfd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "abc60e012d7e4c8dac5250de803d4676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/25], Loss: 14.3454, Train acc: 0.9512, Test acc: 0.5820\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3bb08e022c7041cfb545517f89d5c928",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ddd8987ebf8044b481d89ad566e1362b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/25], Loss: 14.9611, Train acc: 0.9463, Test acc: 0.6189\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4877f85e6c14b199640e414a1b4dc4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd730c5605984823bdd98f13b3de338a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/25], Loss: 14.5157, Train acc: 0.9488, Test acc: 0.6008\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d0286344d0345faba87ba651708d1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9205cb13e60450e88dde8553b9bd96f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/25], Loss: 9.7176, Train acc: 0.9668, Test acc: 0.6156\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5996fdb52c304383868cad6a9b73ac94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac86901967ed4fdb95d1eafcf4acd191",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/25], Loss: 9.4511, Train acc: 0.9697, Test acc: 0.6074\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56bf05cb4b104a9ea8d466e6bbcbe111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "401c5b92d1b645408c79d739725effc7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/25], Loss: 9.0663, Train acc: 0.9695, Test acc: 0.5990\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fcb2acf9973454187e960be8141bae0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4587b500332348f89f5cbafd8d73f757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/25], Loss: 7.8427, Train acc: 0.9735, Test acc: 0.6004\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46f6338f1b1347a7b1ed5052cf57ede7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c34207778a649b6aab754aa4ff0d8f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/25], Loss: 10.0756, Train acc: 0.9674, Test acc: 0.5739\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a094ae893037486c91e681c56e429dcc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "11308a1dc4e543fd84977fa59e6493b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/25], Loss: 9.7966, Train acc: 0.9693, Test acc: 0.5934\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2bdec14ed6cd412688a61e79ee30f0e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896cb1394e4941ff8b569d5141f536b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/25], Loss: 8.4994, Train acc: 0.9704, Test acc: 0.6093\n",
      "Wall time: 56.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for_losses = []\n",
    "n_total_steps = len(train_iter)\n",
    "\n",
    "\n",
    "for epoch in range(25):\n",
    "    total_loss, train_accuracy = train(model, device, train_iter, optimizer, total = math.ceil(len(train_ds)/64))\n",
    "    test_accuracy = test(model, device, test_iter, total = math.ceil(len(test_ds)/64))\n",
    "    for_losses.append(total_loss.cpu().detach().numpy())\n",
    "    print (f'Epoch [{epoch+1}/{25}], Loss: {total_loss:.4f}, Train acc: {train_accuracy:.4f}, Test acc: {test_accuracy:.4f}')                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uo-hf5CQ0iWv"
   },
   "source": [
    "## 3. Классификация обзоров на фильмы (ConvNet)\n",
    "\n",
    "Датасет: https://disk.yandex.ru/d/tdinpb0nN_Dsrg\n",
    "\n",
    "2.1 Создайте набор данных на основе файлов polarity/positive_reviews.csv (положительные отзывы) и polarity/negative_reviews.csv (отрицательные отзывы). Разбейте на обучающую и тестовую выборку.\n",
    "  * токен = __слово__\n",
    "  * данные для обучения в датасете представляются в виде последовательности индексов токенов\n",
    "  * словарь создается на основе _только_ обучающей выборки. Для корректной обработки ситуаций, когда в тестовой выборке встретится токен, который не хранится в словаре, добавьте в словарь специальный токен `<UNK>`\n",
    "  * добавьте предобработку текста\n",
    "\n",
    "2.2. Обучите классификатор.\n",
    "  \n",
    "  * Для преобразования последовательности индексов в последовательность векторов используйте `nn.Embedding` \n",
    "    - подберите адекватную размерность вектора эмбеддинга: \n",
    "    - модуль `nn.Embedding` обучается\n",
    "\n",
    "  * Используйте одномерные свертки и пулинг (`nn.Conv1d`, `nn.MaxPool1d`)\n",
    "    - обратите внимание, что `nn.Conv1d` ожидает на вход трехмерный тензор размерности `(batch, embedding_dim, seq_len)`\n",
    "\n",
    "\n",
    "2.7 Измерить точность на тестовой выборке. Проверить работоспособность модели: придумать небольшой отзыв, прогнать его через модель и вывести номер предсказанного класса (сделать это для явно позитивного и явно негативного отзыва)\n",
    "* Целевое значение accuracy на валидации - 70+%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive = pd.read_csv(\"negative_reviews.txt\", sep=\"\\n\", header=None, names=[\"Review\"])\n",
    "nigative = pd.read_csv(\"positive_reviews.txt\", sep=\"\\n\", header=None, names=[\"Review\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive[\"class\"] = 1\n",
    "nigative[\"class\"] = 0\n",
    "result = pd.concat([positive, nigative], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>the rock is destined to be the 21st century's ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>the gorgeously elaborate continuation of \" the...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>effective but too-tepid biopic</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>if you sometimes like to go to the movies to h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>emerges as something rare , an issue movie tha...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10657</th>\n",
       "      <td>a terrible movie that some people will neverth...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10658</th>\n",
       "      <td>there are many definitions of 'time waster' bu...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10659</th>\n",
       "      <td>as it stands , crocodile hunter has the hurrie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10660</th>\n",
       "      <td>the thing looks like a made-for-home-video qui...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10661</th>\n",
       "      <td>enigma is well-made , but it's just too dry an...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10662 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                  Review  class\n",
       "0      the rock is destined to be the 21st century's ...      1\n",
       "1      the gorgeously elaborate continuation of \" the...      1\n",
       "2                         effective but too-tepid biopic      1\n",
       "3      if you sometimes like to go to the movies to h...      1\n",
       "4      emerges as something rare , an issue movie tha...      1\n",
       "...                                                  ...    ...\n",
       "10657  a terrible movie that some people will neverth...      0\n",
       "10658  there are many definitions of 'time waster' bu...      0\n",
       "10659  as it stands , crocodile hunter has the hurrie...      0\n",
       "10660  the thing looks like a made-for-home-video qui...      0\n",
       "10661  enigma is well-made , but it's just too dry an...      0\n",
       "\n",
       "[10662 rows x 2 columns]"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = result['Review']\n",
    "y = result['class']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UNK',\n",
       " 'the',\n",
       " 'effective',\n",
       " 'if',\n",
       " 'emerges',\n",
       " 'offers',\n",
       " 'perhaps',\n",
       " 'steers',\n",
       " 'take',\n",
       " 'this',\n",
       " 'what',\n",
       " 'wendigo',\n",
       " 'one',\n",
       " 'ultimately',\n",
       " 'an',\n",
       " 'illuminating',\n",
       " 'a',\n",
       " 'with',\n",
       " 'not',\n",
       " 'scores',\n",
       " 'occasionally',\n",
       " 'spiderman',\n",
       " 'at',\n",
       " 'it',\n",
       " 'guaranteed',\n",
       " 'light',\n",
       " 'while',\n",
       " 'though',\n",
       " 'cantet',\n",
       " 'ms',\n",
       " 'like',\n",
       " 'newton',\n",
       " 'fuller',\n",
       " 'compleja',\n",
       " 'karmen',\n",
       " 'gosling',\n",
       " 'tender',\n",
       " 'may',\n",
       " 'in',\n",
       " 'some',\n",
       " 'sandra',\n",
       " 'behind',\n",
       " 'everytime',\n",
       " 'manages',\n",
       " 'singer',\n",
       " 'you',\n",
       " 'whether',\n",
       " 'disney',\n",
       " 'just',\n",
       " 'morton',\n",
       " 'part',\n",
       " 'son',\n",
       " 'haneke',\n",
       " 'absorbing',\n",
       " 'painful',\n",
       " 'together',\n",
       " 'director',\n",
       " 'doug',\n",
       " 'katz',\n",
       " 'writer',\n",
       " 'i',\n",
       " 'chicago',\n",
       " 'steve',\n",
       " 'on',\n",
       " '4',\n",
       " 'edited',\n",
       " 'hip',\n",
       " 'between',\n",
       " 'somewhere',\n",
       " 'digital',\n",
       " 'unlike',\n",
       " 'extreme',\n",
       " 'run',\n",
       " 'merely',\n",
       " 'schweiger',\n",
       " 'gloriously',\n",
       " 'as',\n",
       " 'far',\n",
       " 'waydowntown',\n",
       " 'tim',\n",
       " 'more',\n",
       " 'enormously',\n",
       " 'here',\n",
       " 'must',\n",
       " 'ray',\n",
       " 'everything',\n",
       " 'sweet',\n",
       " 'fans',\n",
       " 'its',\n",
       " 'stephen',\n",
       " 'there',\n",
       " 'remains',\n",
       " 'makmalbaf',\n",
       " 'for',\n",
       " 'most',\n",
       " 'nair',\n",
       " 'rare',\n",
       " 'ranks',\n",
       " 'engagingly',\n",
       " 'thoroughly',\n",
       " 'maybe',\n",
       " 'watching',\n",
       " 'britney',\n",
       " 'those',\n",
       " 'children',\n",
       " 'tadpole',\n",
       " 'my',\n",
       " 'australian',\n",
       " 'without',\n",
       " 'solid',\n",
       " 'narc',\n",
       " 'drumline',\n",
       " 'tsai',\n",
       " 'purely',\n",
       " 'now',\n",
       " 'we',\n",
       " 'daring',\n",
       " 'moore',\n",
       " 'visits',\n",
       " 'mr',\n",
       " 'all',\n",
       " 'despite',\n",
       " 'day',\n",
       " 'polished',\n",
       " 'elling',\n",
       " 'denis',\n",
       " 'remarkably',\n",
       " 'never',\n",
       " 'behan',\n",
       " 'jir',\n",
       " 'provides',\n",
       " 'windtalkers',\n",
       " 'presents',\n",
       " 'benefits',\n",
       " 'using',\n",
       " 'understands',\n",
       " 'has',\n",
       " 'by',\n",
       " 'parker',\n",
       " 'm',\n",
       " 'thankfully',\n",
       " 'who',\n",
       " 'somewhat',\n",
       " 'intriguing',\n",
       " 'legendary',\n",
       " 'even',\n",
       " 'nearly',\n",
       " 'asia',\n",
       " 'roman',\n",
       " 'high',\n",
       " 'finally',\n",
       " 'spielberg',\n",
       " 'post',\n",
       " 'hu',\n",
       " 'easier',\n",
       " 'girls',\n",
       " 'tunney',\n",
       " 'clever',\n",
       " 'always',\n",
       " 'will',\n",
       " 'hands',\n",
       " 'compelling',\n",
       " 'family',\n",
       " 'attal',\n",
       " 'foster',\n",
       " 'is',\n",
       " 'farrell',\n",
       " 'audiences',\n",
       " 'bennett',\n",
       " 'yeah',\n",
       " 'over',\n",
       " 'nicely',\n",
       " 'boisterous',\n",
       " 'celebrated',\n",
       " 'renner',\n",
       " 'energetic',\n",
       " 'star',\n",
       " 'jacquot',\n",
       " 'romanek',\n",
       " 'hey',\n",
       " 'longley',\n",
       " 'although',\n",
       " 'often',\n",
       " 'go',\n",
       " 'george',\n",
       " 'murderous',\n",
       " 'ana',\n",
       " 'anderson',\n",
       " 'bubba',\n",
       " 'bullock',\n",
       " 'kinnear',\n",
       " 'frailty',\n",
       " 'kaufman',\n",
       " 'chalk',\n",
       " 'dark',\n",
       " 'starts',\n",
       " 'grenier',\n",
       " 'long',\n",
       " 'still',\n",
       " 'call',\n",
       " 'notorious',\n",
       " 'stay',\n",
       " 'working',\n",
       " 'when',\n",
       " 'ranging',\n",
       " 'recalls',\n",
       " 'only',\n",
       " 'lends',\n",
       " 'well',\n",
       " 'jason',\n",
       " 'taymor',\n",
       " 'two',\n",
       " 'disturbing',\n",
       " 'mesmerizing',\n",
       " 'villeneuve',\n",
       " 'fisher',\n",
       " 'bourne',\n",
       " 'sound',\n",
       " 'maguire',\n",
       " 'rarely',\n",
       " 'quite',\n",
       " 'few',\n",
       " 'exquisitely',\n",
       " 'stevenson',\n",
       " 'does',\n",
       " 'almost',\n",
       " 'provide',\n",
       " 'too',\n",
       " 'sure',\n",
       " 'daughter',\n",
       " 'zhang',\n",
       " 'breathes',\n",
       " 'both',\n",
       " 'greengrass',\n",
       " 'smith',\n",
       " 'vividly',\n",
       " 'majidi',\n",
       " 'effectively',\n",
       " 'hayek',\n",
       " 'hilarious',\n",
       " 'several',\n",
       " 'led',\n",
       " 'tuck',\n",
       " 'possession',\n",
       " 'having',\n",
       " 'compellingly',\n",
       " 'divine',\n",
       " 'viewed',\n",
       " 'resourceful',\n",
       " 'antwone',\n",
       " 'gai',\n",
       " 'these',\n",
       " 'time',\n",
       " 'haynes',\n",
       " 'sometimes',\n",
       " 'ambition',\n",
       " 'cute',\n",
       " 'very',\n",
       " 'workmanlike',\n",
       " 'branagh',\n",
       " 'skip',\n",
       " 'bow',\n",
       " 'ice',\n",
       " 'claude',\n",
       " 'rich',\n",
       " 'do',\n",
       " 'ford',\n",
       " 'don',\n",
       " 'complex',\n",
       " 'good',\n",
       " 'pray',\n",
       " 'troubling',\n",
       " 'but',\n",
       " 'much',\n",
       " 'anchored',\n",
       " 'awkward',\n",
       " 'sam',\n",
       " 'goyer',\n",
       " 'binoche',\n",
       " 'gets',\n",
       " 'no',\n",
       " 'look',\n",
       " 'tailored',\n",
       " 'isn',\n",
       " 'makes',\n",
       " 'spiced',\n",
       " 'aside',\n",
       " 'watstein',\n",
       " 'reinforces',\n",
       " 'from',\n",
       " 'ferrara',\n",
       " 'ofrece',\n",
       " 'es',\n",
       " 'theirs',\n",
       " 'flavorful',\n",
       " 'happily',\n",
       " 'thurman',\n",
       " 'hits',\n",
       " 'fessenden',\n",
       " 'pretty',\n",
       " 'so',\n",
       " 'woo',\n",
       " 'barney',\n",
       " 'lilia',\n",
       " 'frida',\n",
       " 'charly',\n",
       " 'nervy',\n",
       " 'sharp',\n",
       " 'bright',\n",
       " 'andersson',\n",
       " 'de',\n",
       " 'cho',\n",
       " 'works',\n",
       " 'about',\n",
       " 'desta',\n",
       " '13',\n",
       " 'worth',\n",
       " 'that',\n",
       " 'hawke',\n",
       " 'full',\n",
       " 'thoughtful',\n",
       " 'witty',\n",
       " 'lee',\n",
       " 'enjoy',\n",
       " 'grant',\n",
       " 'westfeldt',\n",
       " 'harsh',\n",
       " 'earns',\n",
       " 'smart',\n",
       " 'lathan',\n",
       " 'merchant',\n",
       " 'davis',\n",
       " 'achieves',\n",
       " 'once',\n",
       " 'auto',\n",
       " 'broomfield',\n",
       " 'eight',\n",
       " 'everywhere',\n",
       " 'baran',\n",
       " 'noyce',\n",
       " 'evokes',\n",
       " 'uno',\n",
       " 'rain',\n",
       " 'droll',\n",
       " 'zany',\n",
       " 'dolgin',\n",
       " 'engrossing',\n",
       " 'funny',\n",
       " 'sits',\n",
       " 'huppert',\n",
       " 'blade',\n",
       " 'nothing',\n",
       " 'greene',\n",
       " 'finds',\n",
       " 'invincible',\n",
       " 'dazzling',\n",
       " 'japan',\n",
       " 'culkin',\n",
       " 'cuts',\n",
       " 'marvelous',\n",
       " 'novak',\n",
       " 'insomnia',\n",
       " 'francophiles',\n",
       " 'sensitive',\n",
       " 'throwing',\n",
       " 'they',\n",
       " 'mixes',\n",
       " 'hawn',\n",
       " 'nicolas',\n",
       " 'reign',\n",
       " 'thrilling',\n",
       " 'genuinely',\n",
       " 'lauren',\n",
       " 'interacting',\n",
       " 'impresses',\n",
       " 'cedar',\n",
       " 'biggie',\n",
       " 'after',\n",
       " 'nolan',\n",
       " 'andy',\n",
       " 'highlights',\n",
       " 'howard',\n",
       " 'superior',\n",
       " 'subversive',\n",
       " 'leave',\n",
       " 'fine',\n",
       " 'human',\n",
       " 'to',\n",
       " 'shainberg',\n",
       " 'sturdy',\n",
       " 'jagger',\n",
       " 'warm',\n",
       " 'escapism',\n",
       " 'u',\n",
       " 'cremaster',\n",
       " 'flawed',\n",
       " 'uses',\n",
       " 'half',\n",
       " 'entertains',\n",
       " 'dazzles',\n",
       " 'visually',\n",
       " 'lawrence',\n",
       " 'charming',\n",
       " 'woody',\n",
       " 'n',\n",
       " 'made',\n",
       " 't',\n",
       " 'add',\n",
       " 'building',\n",
       " 'chilling',\n",
       " 'against',\n",
       " 'huston',\n",
       " 'leigh',\n",
       " 'jose',\n",
       " 'generally',\n",
       " 'ao',\n",
       " 'neither',\n",
       " 'jaglom',\n",
       " 'beautifully',\n",
       " 'crackerjack',\n",
       " 'garc',\n",
       " 'suffers',\n",
       " 'hoffman',\n",
       " '84',\n",
       " 'takes',\n",
       " 'seldahl',\n",
       " 'byler',\n",
       " 'turns',\n",
       " 'bogdanovich',\n",
       " 'people',\n",
       " 'dense',\n",
       " 'lapaglia',\n",
       " 'altogether',\n",
       " 'instead',\n",
       " 'd',\n",
       " 'city',\n",
       " 'looking',\n",
       " 'mafia',\n",
       " 'fun',\n",
       " 'montias',\n",
       " 'falls',\n",
       " 'reggio',\n",
       " 'unfortunately',\n",
       " 'sade',\n",
       " 'thanks',\n",
       " 'nine',\n",
       " 'third',\n",
       " 'inside',\n",
       " 'highbrow',\n",
       " 'maud',\n",
       " 'pacino',\n",
       " 'old',\n",
       " 'ahhhh',\n",
       " 'yakusho',\n",
       " 'ruzowitzky',\n",
       " 'collateral',\n",
       " 'chaiken',\n",
       " 'remember',\n",
       " 'travels',\n",
       " 'apesar',\n",
       " 'awesome',\n",
       " 'bleakly',\n",
       " 'griffiths',\n",
       " 'prancing',\n",
       " 'sexy',\n",
       " 'candid',\n",
       " 'equilibrium',\n",
       " 'creepy',\n",
       " 'martin',\n",
       " 'beneath',\n",
       " 'holm',\n",
       " 'among',\n",
       " 'seldom',\n",
       " 'audrey',\n",
       " 'overall',\n",
       " 'forgettable',\n",
       " 'ramsay',\n",
       " 'cq',\n",
       " 'holy',\n",
       " 'allows',\n",
       " 'maneuvers',\n",
       " 'psychologically',\n",
       " 'birot',\n",
       " 'vera',\n",
       " 'moody',\n",
       " 'true',\n",
       " 'gives',\n",
       " 'macdowell',\n",
       " 'unflinchingly',\n",
       " 'moretti',\n",
       " 'challenging',\n",
       " 'easily',\n",
       " 'charles',\n",
       " 'michael',\n",
       " 'open',\n",
       " 'immersing',\n",
       " 'further',\n",
       " 'lovely',\n",
       " 'displaying',\n",
       " 'passable',\n",
       " 'belongs',\n",
       " 'pumpkin',\n",
       " 'corny',\n",
       " 'feature',\n",
       " 'every',\n",
       " 'harris',\n",
       " 'deliriously',\n",
       " 'trademark',\n",
       " 'liotta',\n",
       " 'contando',\n",
       " 'brian',\n",
       " 'escaping',\n",
       " 'preaches',\n",
       " 'birthday',\n",
       " 'cool',\n",
       " 'drops',\n",
       " 'kept',\n",
       " 'first',\n",
       " 'atom',\n",
       " 'essentially',\n",
       " 'hardly',\n",
       " 'based',\n",
       " 'exciting',\n",
       " 'another',\n",
       " 'viewers',\n",
       " 'jones',\n",
       " 'likely',\n",
       " 'blanchett',\n",
       " 'filmmakers',\n",
       " 'mostly',\n",
       " 'featuring',\n",
       " 'oscar',\n",
       " 'brilliantly',\n",
       " 'lan',\n",
       " 'delightfully',\n",
       " 'richard',\n",
       " 'opening',\n",
       " 'shamelessly',\n",
       " 'allen',\n",
       " 'certainly',\n",
       " 'winds',\n",
       " 'steven',\n",
       " 'laced',\n",
       " 'familiar',\n",
       " 'jeffrey',\n",
       " 'stevens',\n",
       " 'meyjes',\n",
       " 'tian',\n",
       " 'imagine',\n",
       " 'others',\n",
       " 'miller',\n",
       " 'adaptation',\n",
       " 'moonlight',\n",
       " 'robin',\n",
       " 'australia',\n",
       " 'told',\n",
       " 'duvall',\n",
       " 'measured',\n",
       " 'maelstrom',\n",
       " 'poignant',\n",
       " 'enough',\n",
       " 'serious',\n",
       " 'best',\n",
       " 'lookin',\n",
       " 'extremely',\n",
       " 'k',\n",
       " 'cineasts',\n",
       " 'begins',\n",
       " 'creates',\n",
       " 'reno',\n",
       " 'arteta',\n",
       " 'stands',\n",
       " 'filled',\n",
       " 'sparkles',\n",
       " 'proves',\n",
       " 'top',\n",
       " 'beresford',\n",
       " 'probably',\n",
       " 'would',\n",
       " 'sch',\n",
       " 's1m0ne',\n",
       " 'films',\n",
       " 'stuffed',\n",
       " 'affectionately',\n",
       " 'same',\n",
       " 'wilco',\n",
       " 'leguizamo',\n",
       " 'nettelbeck',\n",
       " 'below',\n",
       " 'lightweight',\n",
       " 'predictable',\n",
       " 'whenever',\n",
       " 'highly',\n",
       " 'cusack',\n",
       " 'knows',\n",
       " 'jae',\n",
       " 'malcolm',\n",
       " 'caine',\n",
       " 'reveals',\n",
       " 'simultaneously',\n",
       " 'miyazaki',\n",
       " 'meeting',\n",
       " 'poetry',\n",
       " 'directing',\n",
       " 'amari',\n",
       " 'rabbit',\n",
       " 'strange',\n",
       " 'elegant',\n",
       " 'decasia',\n",
       " 'ryan',\n",
       " 'imperfect',\n",
       " 'passionate',\n",
       " 'feral',\n",
       " 'hollywood',\n",
       " 'perfectly',\n",
       " 'shyamalan',\n",
       " 'raimi',\n",
       " 'boomers',\n",
       " 'real',\n",
       " 'stage',\n",
       " 'sitting',\n",
       " 'strip',\n",
       " 'definitely',\n",
       " 'uplifting',\n",
       " 'heaven',\n",
       " 'stock',\n",
       " 'plays',\n",
       " 'alain',\n",
       " 'melodrama',\n",
       " 'en',\n",
       " 'coppola',\n",
       " 'wang',\n",
       " 'clooney',\n",
       " 'os',\n",
       " 'catch',\n",
       " 'methodical',\n",
       " 'reaches',\n",
       " 'young',\n",
       " 'e',\n",
       " 'helps',\n",
       " 'jackson',\n",
       " 'charlotte',\n",
       " 'defies',\n",
       " 'keenly',\n",
       " 'red',\n",
       " 'succeeds',\n",
       " 'informative',\n",
       " 'gaunt',\n",
       " 'eyre',\n",
       " 'clint',\n",
       " 'quitting',\n",
       " 'rock',\n",
       " 'efficient',\n",
       " 'gorgeous',\n",
       " 'executed',\n",
       " 'asks',\n",
       " 'darkly',\n",
       " 'stanley',\n",
       " 'norton',\n",
       " 'enjoyably',\n",
       " 'isabelle',\n",
       " 'wedding',\n",
       " 'brings',\n",
       " 'runs',\n",
       " 'phillip',\n",
       " 'wise',\n",
       " 'whatever',\n",
       " 'mastering',\n",
       " 'combine',\n",
       " 'flamboyant',\n",
       " 'surprisingly',\n",
       " 'apart',\n",
       " 'morvern',\n",
       " 'wiseman',\n",
       " 'arguably',\n",
       " 'invigorating',\n",
       " 'maintains',\n",
       " 'berry',\n",
       " 'caviezel',\n",
       " 'b',\n",
       " 'muccino',\n",
       " 'ozpetek',\n",
       " 'nicole',\n",
       " 'life',\n",
       " 'exudes',\n",
       " 'angela',\n",
       " 'transcends',\n",
       " 'spare',\n",
       " 'think',\n",
       " 'giggling',\n",
       " 'drug',\n",
       " 'efteriades',\n",
       " 'amazing',\n",
       " 'shattering',\n",
       " 'douglas',\n",
       " 'able',\n",
       " 'gently',\n",
       " 'ultimate',\n",
       " 'lohman',\n",
       " 'gra',\n",
       " 'affirms',\n",
       " 'deepa',\n",
       " 'pan',\n",
       " 'dogtown',\n",
       " 'plenty',\n",
       " 'could',\n",
       " 'danny',\n",
       " 'williams',\n",
       " 'twist',\n",
       " 'thought',\n",
       " 'hatosy',\n",
       " 'uneven',\n",
       " 'alan',\n",
       " 'fresh',\n",
       " 'toes',\n",
       " 'whereas',\n",
       " 'triple',\n",
       " 'likeable',\n",
       " 'secretary',\n",
       " 'judith',\n",
       " 'photographed',\n",
       " 'confessions',\n",
       " 'simple',\n",
       " 'less',\n",
       " 'notwithstanding',\n",
       " 'functions',\n",
       " 'terry',\n",
       " 'parts',\n",
       " 'reassuring',\n",
       " 'armed',\n",
       " 'enticing',\n",
       " 'going',\n",
       " 'almodovar',\n",
       " 'better',\n",
       " 'tense',\n",
       " 'hard',\n",
       " 'me',\n",
       " 'inherently',\n",
       " 'pure',\n",
       " 'fast',\n",
       " 'devos',\n",
       " 'fulfills',\n",
       " 'wickedly',\n",
       " 'until',\n",
       " 'fincher',\n",
       " 'exploits',\n",
       " 'ratliff',\n",
       " 'passions',\n",
       " 'contrived',\n",
       " 'lavishly',\n",
       " 'combines',\n",
       " 'scooby',\n",
       " 'remove',\n",
       " 'gooding',\n",
       " 'see',\n",
       " 'features',\n",
       " 'bravo',\n",
       " 'lead',\n",
       " 'rife',\n",
       " 'rubbo',\n",
       " 'overcomes',\n",
       " 'touch',\n",
       " 'astonishingly',\n",
       " 'fubar',\n",
       " 'slight',\n",
       " 'unfolds',\n",
       " 'worse',\n",
       " 'canada',\n",
       " 'alternately',\n",
       " 'yes',\n",
       " 'hartley',\n",
       " 'insanely',\n",
       " 'anyone',\n",
       " 'bowling',\n",
       " 'talk',\n",
       " 'white',\n",
       " 'refreshing',\n",
       " 'return',\n",
       " 'suffice',\n",
       " 'marvelously',\n",
       " 'divertida',\n",
       " 'deserves',\n",
       " 'confounding',\n",
       " 'fulford',\n",
       " 'featherweight',\n",
       " 'diggs',\n",
       " 'entirely',\n",
       " 'terrific',\n",
       " 'wonderful',\n",
       " 'ian',\n",
       " 'sports',\n",
       " 'directors',\n",
       " 'labute',\n",
       " 'irwin',\n",
       " 'film',\n",
       " 'contradicts',\n",
       " 'filmmaker',\n",
       " 'rises',\n",
       " 'kids',\n",
       " 'leaping',\n",
       " 'sean',\n",
       " 'she',\n",
       " 'payne',\n",
       " 'try',\n",
       " 'slow',\n",
       " 'austin',\n",
       " 'elvira',\n",
       " 'hashiguchi',\n",
       " 'f',\n",
       " 'shiri',\n",
       " 'jeong',\n",
       " 'grainy',\n",
       " 'daily',\n",
       " 'sex',\n",
       " 'compared',\n",
       " 'delivers',\n",
       " 'vincent',\n",
       " 'fierce',\n",
       " 'cletis',\n",
       " 'campanella',\n",
       " 'either',\n",
       " 'gripping',\n",
       " 'standing',\n",
       " 'rehearsals',\n",
       " 'your',\n",
       " 'bloody',\n",
       " 'faithful',\n",
       " 'intelligent',\n",
       " 'fathers',\n",
       " 'last',\n",
       " 'el',\n",
       " 'can',\n",
       " 'goes',\n",
       " 'unexpected',\n",
       " 'audiard',\n",
       " 'credit',\n",
       " 'witherspoon',\n",
       " 'ambitious',\n",
       " 'eric',\n",
       " 'blithely',\n",
       " 'triumph',\n",
       " 'discursive',\n",
       " 'salma',\n",
       " 'deflated',\n",
       " 'performances',\n",
       " 'undoubtedly',\n",
       " 'personal',\n",
       " 'throughout',\n",
       " 'flat',\n",
       " 'nachtwey',\n",
       " 'robert',\n",
       " 'american',\n",
       " 'fontaine',\n",
       " 'p',\n",
       " 'twenty',\n",
       " 'schepisi',\n",
       " 'lejos',\n",
       " 'interesante',\n",
       " 'got',\n",
       " 'astonishing',\n",
       " 'land',\n",
       " 'blue',\n",
       " 'deliciously',\n",
       " 'suspend',\n",
       " 'trades',\n",
       " 'implicitly',\n",
       " 'victor',\n",
       " 'topics',\n",
       " 'none',\n",
       " 'diane',\n",
       " 'takashi',\n",
       " 'exhilarating',\n",
       " 'because',\n",
       " 'originality',\n",
       " 'walter',\n",
       " 'creeps',\n",
       " 'spider',\n",
       " 'si',\n",
       " 'saddled',\n",
       " 'russell',\n",
       " 'otto',\n",
       " 'demonstrates',\n",
       " 'expect',\n",
       " 'cox',\n",
       " 'halloween',\n",
       " 'romantic',\n",
       " 'bouquet',\n",
       " 'barbershop',\n",
       " 'tully',\n",
       " 'kiarostami',\n",
       " 'murder',\n",
       " 'texan',\n",
       " 'mama',\n",
       " 'seeing',\n",
       " 'speaks',\n",
       " 'hailed',\n",
       " 'be',\n",
       " 'thumbs',\n",
       " 'short',\n",
       " 'kosminsky',\n",
       " 'bound',\n",
       " 'vampire',\n",
       " 'jackie',\n",
       " 'brown',\n",
       " 'assured',\n",
       " 'para',\n",
       " 'neatly',\n",
       " 'frequent',\n",
       " '10',\n",
       " 'cube',\n",
       " 'barry',\n",
       " 'changing',\n",
       " 'borrows',\n",
       " 'puts',\n",
       " 'intimate',\n",
       " 'excellent',\n",
       " 'jolting',\n",
       " 'close',\n",
       " 'skillfully',\n",
       " 'damon',\n",
       " 'beautiful',\n",
       " 'transforms',\n",
       " 'sayles',\n",
       " 'solondz',\n",
       " 'captures',\n",
       " 'imamura',\n",
       " 'exactly',\n",
       " 'rogers',\n",
       " 'deliberately',\n",
       " 'nicholson',\n",
       " 'plummer',\n",
       " 'estupendamente',\n",
       " 'rodriguez',\n",
       " 'amy',\n",
       " 'garcia',\n",
       " 'heartwarming',\n",
       " 'home',\n",
       " 'scott',\n",
       " 'wallace',\n",
       " 'marshall',\n",
       " 'soderbergh',\n",
       " 'hatfield',\n",
       " 'infidelity',\n",
       " 'audacious',\n",
       " 'humorous',\n",
       " 'rouge',\n",
       " 'o',\n",
       " 'feardotcom',\n",
       " 'stuart',\n",
       " 'boasts',\n",
       " 'nakata',\n",
       " 'satin',\n",
       " 'cannon',\n",
       " 'ok',\n",
       " 'craig',\n",
       " 'sorvino',\n",
       " 'great',\n",
       " 'qutting',\n",
       " 'spellbinding',\n",
       " 'impossible',\n",
       " 'many',\n",
       " 'henry',\n",
       " 'playing',\n",
       " 'griffin',\n",
       " 'lacks',\n",
       " 'brims',\n",
       " 'fudges',\n",
       " 'nohe',\n",
       " 'secret',\n",
       " 'weird',\n",
       " 'moving',\n",
       " 'brave',\n",
       " 'sad',\n",
       " 'seeks',\n",
       " 'mordantly',\n",
       " 'earnest',\n",
       " 'vibrantly',\n",
       " 'sweetly',\n",
       " 'bursting',\n",
       " 'kudos',\n",
       " 'colorful',\n",
       " 'westbrook',\n",
       " 'harmless',\n",
       " 'alternates',\n",
       " 'samuel',\n",
       " 'trapped',\n",
       " 'maid',\n",
       " 'daringly',\n",
       " 'hugely',\n",
       " 'swimming',\n",
       " 'blessed',\n",
       " 'turturro',\n",
       " 'meant',\n",
       " 'simply',\n",
       " 'fred',\n",
       " 'each',\n",
       " 'horns',\n",
       " 'oddly',\n",
       " 'captivates',\n",
       " 'how',\n",
       " ...]"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = []\n",
    "for i in X:\n",
    "    i = re.sub(r\"[^a-zA-Z0-9\\\\]\", \" \", i)\n",
    "    res = word_tokenize(i)\n",
    "    self.max_seq_len = max(self.max_seq_len, len(res))\n",
    "    words.append(res)\n",
    "words = ['UNK']+[i[0] for i in words if i not in stopwords.words('english')]\n",
    "uniqueWords = [] \n",
    "for i in words:\n",
    "    if not i in uniqueWords:\n",
    "        uniqueWords.append(i)\n",
    "uniqueWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2175"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(uniqueWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "      def __init__(self, data):\n",
    "                words = []\n",
    "                self.max_seq_len = 0\n",
    "                for i in data:\n",
    "                    k = i\n",
    "                    kk = nltk.word_tokenize(k)\n",
    "                    self.max_seq_len = max(self.max_seq_len, len(kk))\n",
    "                    i = re.sub(r\"[^a-zA-Z0-9\\\\]\", \" \", i)\n",
    "                    res = word_tokenize(i)\n",
    "                    words.append(res)\n",
    "                words = ['UNK']+[i[0] for i in words if i not in stopwords.words('english')]\n",
    "                uniqueWords = [] \n",
    "                for i in words:\n",
    "                    if not i in uniqueWords:\n",
    "                        uniqueWords.append(i)\n",
    "                self.idx_to_token = {idx: val for idx, val in enumerate(uniqueWords)}\n",
    "                self.token_to_idx = {val: idx for idx, val in self.idx_to_token.items()}\n",
    "                self.vocab_len = len(self.idx_to_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'UNK',\n",
       " 1: 'the',\n",
       " 2: 'effective',\n",
       " 3: 'if',\n",
       " 4: 'emerges',\n",
       " 5: 'offers',\n",
       " 6: 'perhaps',\n",
       " 7: 'steers',\n",
       " 8: 'take',\n",
       " 9: 'this',\n",
       " 10: 'what',\n",
       " 11: 'wendigo',\n",
       " 12: 'one',\n",
       " 13: 'ultimately',\n",
       " 14: 'an',\n",
       " 15: 'illuminating',\n",
       " 16: 'a',\n",
       " 17: 'with',\n",
       " 18: 'not',\n",
       " 19: 'scores',\n",
       " 20: 'occasionally',\n",
       " 21: 'spiderman',\n",
       " 22: 'at',\n",
       " 23: 'it',\n",
       " 24: 'guaranteed',\n",
       " 25: 'light',\n",
       " 26: 'while',\n",
       " 27: 'though',\n",
       " 28: 'cantet',\n",
       " 29: 'ms',\n",
       " 30: 'like',\n",
       " 31: 'newton',\n",
       " 32: 'fuller',\n",
       " 33: 'compleja',\n",
       " 34: 'karmen',\n",
       " 35: 'gosling',\n",
       " 36: 'tender',\n",
       " 37: 'may',\n",
       " 38: 'in',\n",
       " 39: 'some',\n",
       " 40: 'sandra',\n",
       " 41: 'behind',\n",
       " 42: 'everytime',\n",
       " 43: 'manages',\n",
       " 44: 'singer',\n",
       " 45: 'you',\n",
       " 46: 'whether',\n",
       " 47: 'disney',\n",
       " 48: 'just',\n",
       " 49: 'morton',\n",
       " 50: 'part',\n",
       " 51: 'son',\n",
       " 52: 'haneke',\n",
       " 53: 'absorbing',\n",
       " 54: 'painful',\n",
       " 55: 'together',\n",
       " 56: 'director',\n",
       " 57: 'doug',\n",
       " 58: 'katz',\n",
       " 59: 'writer',\n",
       " 60: 'i',\n",
       " 61: 'chicago',\n",
       " 62: 'steve',\n",
       " 63: 'on',\n",
       " 64: '4',\n",
       " 65: 'edited',\n",
       " 66: 'hip',\n",
       " 67: 'between',\n",
       " 68: 'somewhere',\n",
       " 69: 'digital',\n",
       " 70: 'unlike',\n",
       " 71: 'extreme',\n",
       " 72: 'run',\n",
       " 73: 'merely',\n",
       " 74: 'schweiger',\n",
       " 75: 'gloriously',\n",
       " 76: 'as',\n",
       " 77: 'far',\n",
       " 78: 'waydowntown',\n",
       " 79: 'tim',\n",
       " 80: 'more',\n",
       " 81: 'enormously',\n",
       " 82: 'here',\n",
       " 83: 'must',\n",
       " 84: 'ray',\n",
       " 85: 'everything',\n",
       " 86: 'sweet',\n",
       " 87: 'fans',\n",
       " 88: 'its',\n",
       " 89: 'stephen',\n",
       " 90: 'there',\n",
       " 91: 'remains',\n",
       " 92: 'makmalbaf',\n",
       " 93: 'for',\n",
       " 94: 'most',\n",
       " 95: 'nair',\n",
       " 96: 'rare',\n",
       " 97: 'ranks',\n",
       " 98: 'engagingly',\n",
       " 99: 'thoroughly',\n",
       " 100: 'maybe',\n",
       " 101: 'watching',\n",
       " 102: 'britney',\n",
       " 103: 'those',\n",
       " 104: 'children',\n",
       " 105: 'tadpole',\n",
       " 106: 'my',\n",
       " 107: 'australian',\n",
       " 108: 'without',\n",
       " 109: 'solid',\n",
       " 110: 'narc',\n",
       " 111: 'drumline',\n",
       " 112: 'tsai',\n",
       " 113: 'purely',\n",
       " 114: 'now',\n",
       " 115: 'we',\n",
       " 116: 'daring',\n",
       " 117: 'moore',\n",
       " 118: 'visits',\n",
       " 119: 'mr',\n",
       " 120: 'all',\n",
       " 121: 'despite',\n",
       " 122: 'day',\n",
       " 123: 'polished',\n",
       " 124: 'elling',\n",
       " 125: 'denis',\n",
       " 126: 'remarkably',\n",
       " 127: 'never',\n",
       " 128: 'behan',\n",
       " 129: 'jir',\n",
       " 130: 'provides',\n",
       " 131: 'windtalkers',\n",
       " 132: 'presents',\n",
       " 133: 'benefits',\n",
       " 134: 'using',\n",
       " 135: 'understands',\n",
       " 136: 'has',\n",
       " 137: 'by',\n",
       " 138: 'parker',\n",
       " 139: 'm',\n",
       " 140: 'thankfully',\n",
       " 141: 'who',\n",
       " 142: 'somewhat',\n",
       " 143: 'intriguing',\n",
       " 144: 'legendary',\n",
       " 145: 'even',\n",
       " 146: 'nearly',\n",
       " 147: 'asia',\n",
       " 148: 'roman',\n",
       " 149: 'high',\n",
       " 150: 'finally',\n",
       " 151: 'spielberg',\n",
       " 152: 'post',\n",
       " 153: 'hu',\n",
       " 154: 'easier',\n",
       " 155: 'girls',\n",
       " 156: 'tunney',\n",
       " 157: 'clever',\n",
       " 158: 'always',\n",
       " 159: 'will',\n",
       " 160: 'hands',\n",
       " 161: 'compelling',\n",
       " 162: 'family',\n",
       " 163: 'attal',\n",
       " 164: 'foster',\n",
       " 165: 'is',\n",
       " 166: 'farrell',\n",
       " 167: 'audiences',\n",
       " 168: 'bennett',\n",
       " 169: 'yeah',\n",
       " 170: 'over',\n",
       " 171: 'nicely',\n",
       " 172: 'boisterous',\n",
       " 173: 'celebrated',\n",
       " 174: 'renner',\n",
       " 175: 'energetic',\n",
       " 176: 'star',\n",
       " 177: 'jacquot',\n",
       " 178: 'romanek',\n",
       " 179: 'hey',\n",
       " 180: 'longley',\n",
       " 181: 'although',\n",
       " 182: 'often',\n",
       " 183: 'go',\n",
       " 184: 'george',\n",
       " 185: 'murderous',\n",
       " 186: 'ana',\n",
       " 187: 'anderson',\n",
       " 188: 'bubba',\n",
       " 189: 'bullock',\n",
       " 190: 'kinnear',\n",
       " 191: 'frailty',\n",
       " 192: 'kaufman',\n",
       " 193: 'chalk',\n",
       " 194: 'dark',\n",
       " 195: 'starts',\n",
       " 196: 'grenier',\n",
       " 197: 'long',\n",
       " 198: 'still',\n",
       " 199: 'call',\n",
       " 200: 'notorious',\n",
       " 201: 'stay',\n",
       " 202: 'working',\n",
       " 203: 'when',\n",
       " 204: 'ranging',\n",
       " 205: 'recalls',\n",
       " 206: 'only',\n",
       " 207: 'lends',\n",
       " 208: 'well',\n",
       " 209: 'jason',\n",
       " 210: 'taymor',\n",
       " 211: 'two',\n",
       " 212: 'disturbing',\n",
       " 213: 'mesmerizing',\n",
       " 214: 'villeneuve',\n",
       " 215: 'fisher',\n",
       " 216: 'bourne',\n",
       " 217: 'sound',\n",
       " 218: 'maguire',\n",
       " 219: 'rarely',\n",
       " 220: 'quite',\n",
       " 221: 'few',\n",
       " 222: 'exquisitely',\n",
       " 223: 'stevenson',\n",
       " 224: 'does',\n",
       " 225: 'almost',\n",
       " 226: 'provide',\n",
       " 227: 'too',\n",
       " 228: 'sure',\n",
       " 229: 'daughter',\n",
       " 230: 'zhang',\n",
       " 231: 'breathes',\n",
       " 232: 'both',\n",
       " 233: 'greengrass',\n",
       " 234: 'smith',\n",
       " 235: 'vividly',\n",
       " 236: 'majidi',\n",
       " 237: 'effectively',\n",
       " 238: 'hayek',\n",
       " 239: 'hilarious',\n",
       " 240: 'several',\n",
       " 241: 'led',\n",
       " 242: 'tuck',\n",
       " 243: 'possession',\n",
       " 244: 'having',\n",
       " 245: 'compellingly',\n",
       " 246: 'divine',\n",
       " 247: 'viewed',\n",
       " 248: 'resourceful',\n",
       " 249: 'antwone',\n",
       " 250: 'gai',\n",
       " 251: 'these',\n",
       " 252: 'time',\n",
       " 253: 'haynes',\n",
       " 254: 'sometimes',\n",
       " 255: 'ambition',\n",
       " 256: 'cute',\n",
       " 257: 'very',\n",
       " 258: 'workmanlike',\n",
       " 259: 'branagh',\n",
       " 260: 'skip',\n",
       " 261: 'bow',\n",
       " 262: 'ice',\n",
       " 263: 'claude',\n",
       " 264: 'rich',\n",
       " 265: 'do',\n",
       " 266: 'ford',\n",
       " 267: 'don',\n",
       " 268: 'complex',\n",
       " 269: 'good',\n",
       " 270: 'pray',\n",
       " 271: 'troubling',\n",
       " 272: 'but',\n",
       " 273: 'much',\n",
       " 274: 'anchored',\n",
       " 275: 'awkward',\n",
       " 276: 'sam',\n",
       " 277: 'goyer',\n",
       " 278: 'binoche',\n",
       " 279: 'gets',\n",
       " 280: 'no',\n",
       " 281: 'look',\n",
       " 282: 'tailored',\n",
       " 283: 'isn',\n",
       " 284: 'makes',\n",
       " 285: 'spiced',\n",
       " 286: 'aside',\n",
       " 287: 'watstein',\n",
       " 288: 'reinforces',\n",
       " 289: 'from',\n",
       " 290: 'ferrara',\n",
       " 291: 'ofrece',\n",
       " 292: 'es',\n",
       " 293: 'theirs',\n",
       " 294: 'flavorful',\n",
       " 295: 'happily',\n",
       " 296: 'thurman',\n",
       " 297: 'hits',\n",
       " 298: 'fessenden',\n",
       " 299: 'pretty',\n",
       " 300: 'so',\n",
       " 301: 'woo',\n",
       " 302: 'barney',\n",
       " 303: 'lilia',\n",
       " 304: 'frida',\n",
       " 305: 'charly',\n",
       " 306: 'nervy',\n",
       " 307: 'sharp',\n",
       " 308: 'bright',\n",
       " 309: 'andersson',\n",
       " 310: 'de',\n",
       " 311: 'cho',\n",
       " 312: 'works',\n",
       " 313: 'about',\n",
       " 314: 'desta',\n",
       " 315: '13',\n",
       " 316: 'worth',\n",
       " 317: 'that',\n",
       " 318: 'hawke',\n",
       " 319: 'full',\n",
       " 320: 'thoughtful',\n",
       " 321: 'witty',\n",
       " 322: 'lee',\n",
       " 323: 'enjoy',\n",
       " 324: 'grant',\n",
       " 325: 'westfeldt',\n",
       " 326: 'harsh',\n",
       " 327: 'earns',\n",
       " 328: 'smart',\n",
       " 329: 'lathan',\n",
       " 330: 'merchant',\n",
       " 331: 'davis',\n",
       " 332: 'achieves',\n",
       " 333: 'once',\n",
       " 334: 'auto',\n",
       " 335: 'broomfield',\n",
       " 336: 'eight',\n",
       " 337: 'everywhere',\n",
       " 338: 'baran',\n",
       " 339: 'noyce',\n",
       " 340: 'evokes',\n",
       " 341: 'uno',\n",
       " 342: 'rain',\n",
       " 343: 'droll',\n",
       " 344: 'zany',\n",
       " 345: 'dolgin',\n",
       " 346: 'engrossing',\n",
       " 347: 'funny',\n",
       " 348: 'sits',\n",
       " 349: 'huppert',\n",
       " 350: 'blade',\n",
       " 351: 'nothing',\n",
       " 352: 'greene',\n",
       " 353: 'finds',\n",
       " 354: 'invincible',\n",
       " 355: 'dazzling',\n",
       " 356: 'japan',\n",
       " 357: 'culkin',\n",
       " 358: 'cuts',\n",
       " 359: 'marvelous',\n",
       " 360: 'novak',\n",
       " 361: 'insomnia',\n",
       " 362: 'francophiles',\n",
       " 363: 'sensitive',\n",
       " 364: 'throwing',\n",
       " 365: 'they',\n",
       " 366: 'mixes',\n",
       " 367: 'hawn',\n",
       " 368: 'nicolas',\n",
       " 369: 'reign',\n",
       " 370: 'thrilling',\n",
       " 371: 'genuinely',\n",
       " 372: 'lauren',\n",
       " 373: 'interacting',\n",
       " 374: 'impresses',\n",
       " 375: 'cedar',\n",
       " 376: 'biggie',\n",
       " 377: 'after',\n",
       " 378: 'nolan',\n",
       " 379: 'andy',\n",
       " 380: 'highlights',\n",
       " 381: 'howard',\n",
       " 382: 'superior',\n",
       " 383: 'subversive',\n",
       " 384: 'leave',\n",
       " 385: 'fine',\n",
       " 386: 'human',\n",
       " 387: 'to',\n",
       " 388: 'shainberg',\n",
       " 389: 'sturdy',\n",
       " 390: 'jagger',\n",
       " 391: 'warm',\n",
       " 392: 'escapism',\n",
       " 393: 'u',\n",
       " 394: 'cremaster',\n",
       " 395: 'flawed',\n",
       " 396: 'uses',\n",
       " 397: 'half',\n",
       " 398: 'entertains',\n",
       " 399: 'dazzles',\n",
       " 400: 'visually',\n",
       " 401: 'lawrence',\n",
       " 402: 'charming',\n",
       " 403: 'woody',\n",
       " 404: 'n',\n",
       " 405: 'made',\n",
       " 406: 't',\n",
       " 407: 'add',\n",
       " 408: 'building',\n",
       " 409: 'chilling',\n",
       " 410: 'against',\n",
       " 411: 'huston',\n",
       " 412: 'leigh',\n",
       " 413: 'jose',\n",
       " 414: 'generally',\n",
       " 415: 'ao',\n",
       " 416: 'neither',\n",
       " 417: 'jaglom',\n",
       " 418: 'beautifully',\n",
       " 419: 'crackerjack',\n",
       " 420: 'garc',\n",
       " 421: 'suffers',\n",
       " 422: 'hoffman',\n",
       " 423: '84',\n",
       " 424: 'takes',\n",
       " 425: 'seldahl',\n",
       " 426: 'byler',\n",
       " 427: 'turns',\n",
       " 428: 'bogdanovich',\n",
       " 429: 'people',\n",
       " 430: 'dense',\n",
       " 431: 'lapaglia',\n",
       " 432: 'altogether',\n",
       " 433: 'instead',\n",
       " 434: 'd',\n",
       " 435: 'city',\n",
       " 436: 'looking',\n",
       " 437: 'mafia',\n",
       " 438: 'fun',\n",
       " 439: 'montias',\n",
       " 440: 'falls',\n",
       " 441: 'reggio',\n",
       " 442: 'unfortunately',\n",
       " 443: 'sade',\n",
       " 444: 'thanks',\n",
       " 445: 'nine',\n",
       " 446: 'third',\n",
       " 447: 'inside',\n",
       " 448: 'highbrow',\n",
       " 449: 'maud',\n",
       " 450: 'pacino',\n",
       " 451: 'old',\n",
       " 452: 'ahhhh',\n",
       " 453: 'yakusho',\n",
       " 454: 'ruzowitzky',\n",
       " 455: 'collateral',\n",
       " 456: 'chaiken',\n",
       " 457: 'remember',\n",
       " 458: 'travels',\n",
       " 459: 'apesar',\n",
       " 460: 'awesome',\n",
       " 461: 'bleakly',\n",
       " 462: 'griffiths',\n",
       " 463: 'prancing',\n",
       " 464: 'sexy',\n",
       " 465: 'candid',\n",
       " 466: 'equilibrium',\n",
       " 467: 'creepy',\n",
       " 468: 'martin',\n",
       " 469: 'beneath',\n",
       " 470: 'holm',\n",
       " 471: 'among',\n",
       " 472: 'seldom',\n",
       " 473: 'audrey',\n",
       " 474: 'overall',\n",
       " 475: 'forgettable',\n",
       " 476: 'ramsay',\n",
       " 477: 'cq',\n",
       " 478: 'holy',\n",
       " 479: 'allows',\n",
       " 480: 'maneuvers',\n",
       " 481: 'psychologically',\n",
       " 482: 'birot',\n",
       " 483: 'vera',\n",
       " 484: 'moody',\n",
       " 485: 'true',\n",
       " 486: 'gives',\n",
       " 487: 'macdowell',\n",
       " 488: 'unflinchingly',\n",
       " 489: 'moretti',\n",
       " 490: 'challenging',\n",
       " 491: 'easily',\n",
       " 492: 'charles',\n",
       " 493: 'michael',\n",
       " 494: 'open',\n",
       " 495: 'immersing',\n",
       " 496: 'further',\n",
       " 497: 'lovely',\n",
       " 498: 'displaying',\n",
       " 499: 'passable',\n",
       " 500: 'belongs',\n",
       " 501: 'pumpkin',\n",
       " 502: 'corny',\n",
       " 503: 'feature',\n",
       " 504: 'every',\n",
       " 505: 'harris',\n",
       " 506: 'deliriously',\n",
       " 507: 'trademark',\n",
       " 508: 'liotta',\n",
       " 509: 'contando',\n",
       " 510: 'brian',\n",
       " 511: 'escaping',\n",
       " 512: 'preaches',\n",
       " 513: 'birthday',\n",
       " 514: 'cool',\n",
       " 515: 'drops',\n",
       " 516: 'kept',\n",
       " 517: 'first',\n",
       " 518: 'atom',\n",
       " 519: 'essentially',\n",
       " 520: 'hardly',\n",
       " 521: 'based',\n",
       " 522: 'exciting',\n",
       " 523: 'another',\n",
       " 524: 'viewers',\n",
       " 525: 'jones',\n",
       " 526: 'likely',\n",
       " 527: 'blanchett',\n",
       " 528: 'filmmakers',\n",
       " 529: 'mostly',\n",
       " 530: 'featuring',\n",
       " 531: 'oscar',\n",
       " 532: 'brilliantly',\n",
       " 533: 'lan',\n",
       " 534: 'delightfully',\n",
       " 535: 'richard',\n",
       " 536: 'opening',\n",
       " 537: 'shamelessly',\n",
       " 538: 'allen',\n",
       " 539: 'certainly',\n",
       " 540: 'winds',\n",
       " 541: 'steven',\n",
       " 542: 'laced',\n",
       " 543: 'familiar',\n",
       " 544: 'jeffrey',\n",
       " 545: 'stevens',\n",
       " 546: 'meyjes',\n",
       " 547: 'tian',\n",
       " 548: 'imagine',\n",
       " 549: 'others',\n",
       " 550: 'miller',\n",
       " 551: 'adaptation',\n",
       " 552: 'moonlight',\n",
       " 553: 'robin',\n",
       " 554: 'australia',\n",
       " 555: 'told',\n",
       " 556: 'duvall',\n",
       " 557: 'measured',\n",
       " 558: 'maelstrom',\n",
       " 559: 'poignant',\n",
       " 560: 'enough',\n",
       " 561: 'serious',\n",
       " 562: 'best',\n",
       " 563: 'lookin',\n",
       " 564: 'extremely',\n",
       " 565: 'k',\n",
       " 566: 'cineasts',\n",
       " 567: 'begins',\n",
       " 568: 'creates',\n",
       " 569: 'reno',\n",
       " 570: 'arteta',\n",
       " 571: 'stands',\n",
       " 572: 'filled',\n",
       " 573: 'sparkles',\n",
       " 574: 'proves',\n",
       " 575: 'top',\n",
       " 576: 'beresford',\n",
       " 577: 'probably',\n",
       " 578: 'would',\n",
       " 579: 'sch',\n",
       " 580: 's1m0ne',\n",
       " 581: 'films',\n",
       " 582: 'stuffed',\n",
       " 583: 'affectionately',\n",
       " 584: 'same',\n",
       " 585: 'wilco',\n",
       " 586: 'leguizamo',\n",
       " 587: 'nettelbeck',\n",
       " 588: 'below',\n",
       " 589: 'lightweight',\n",
       " 590: 'predictable',\n",
       " 591: 'whenever',\n",
       " 592: 'highly',\n",
       " 593: 'cusack',\n",
       " 594: 'knows',\n",
       " 595: 'jae',\n",
       " 596: 'malcolm',\n",
       " 597: 'caine',\n",
       " 598: 'reveals',\n",
       " 599: 'simultaneously',\n",
       " 600: 'miyazaki',\n",
       " 601: 'meeting',\n",
       " 602: 'poetry',\n",
       " 603: 'directing',\n",
       " 604: 'amari',\n",
       " 605: 'rabbit',\n",
       " 606: 'strange',\n",
       " 607: 'elegant',\n",
       " 608: 'decasia',\n",
       " 609: 'ryan',\n",
       " 610: 'imperfect',\n",
       " 611: 'passionate',\n",
       " 612: 'feral',\n",
       " 613: 'hollywood',\n",
       " 614: 'perfectly',\n",
       " 615: 'shyamalan',\n",
       " 616: 'raimi',\n",
       " 617: 'boomers',\n",
       " 618: 'real',\n",
       " 619: 'stage',\n",
       " 620: 'sitting',\n",
       " 621: 'strip',\n",
       " 622: 'definitely',\n",
       " 623: 'uplifting',\n",
       " 624: 'heaven',\n",
       " 625: 'stock',\n",
       " 626: 'plays',\n",
       " 627: 'alain',\n",
       " 628: 'melodrama',\n",
       " 629: 'en',\n",
       " 630: 'coppola',\n",
       " 631: 'wang',\n",
       " 632: 'clooney',\n",
       " 633: 'os',\n",
       " 634: 'catch',\n",
       " 635: 'methodical',\n",
       " 636: 'reaches',\n",
       " 637: 'young',\n",
       " 638: 'e',\n",
       " 639: 'helps',\n",
       " 640: 'jackson',\n",
       " 641: 'charlotte',\n",
       " 642: 'defies',\n",
       " 643: 'keenly',\n",
       " 644: 'red',\n",
       " 645: 'succeeds',\n",
       " 646: 'informative',\n",
       " 647: 'gaunt',\n",
       " 648: 'eyre',\n",
       " 649: 'clint',\n",
       " 650: 'quitting',\n",
       " 651: 'rock',\n",
       " 652: 'efficient',\n",
       " 653: 'gorgeous',\n",
       " 654: 'executed',\n",
       " 655: 'asks',\n",
       " 656: 'darkly',\n",
       " 657: 'stanley',\n",
       " 658: 'norton',\n",
       " 659: 'enjoyably',\n",
       " 660: 'isabelle',\n",
       " 661: 'wedding',\n",
       " 662: 'brings',\n",
       " 663: 'runs',\n",
       " 664: 'phillip',\n",
       " 665: 'wise',\n",
       " 666: 'whatever',\n",
       " 667: 'mastering',\n",
       " 668: 'combine',\n",
       " 669: 'flamboyant',\n",
       " 670: 'surprisingly',\n",
       " 671: 'apart',\n",
       " 672: 'morvern',\n",
       " 673: 'wiseman',\n",
       " 674: 'arguably',\n",
       " 675: 'invigorating',\n",
       " 676: 'maintains',\n",
       " 677: 'berry',\n",
       " 678: 'caviezel',\n",
       " 679: 'b',\n",
       " 680: 'muccino',\n",
       " 681: 'ozpetek',\n",
       " 682: 'nicole',\n",
       " 683: 'life',\n",
       " 684: 'exudes',\n",
       " 685: 'angela',\n",
       " 686: 'transcends',\n",
       " 687: 'spare',\n",
       " 688: 'think',\n",
       " 689: 'giggling',\n",
       " 690: 'drug',\n",
       " 691: 'efteriades',\n",
       " 692: 'amazing',\n",
       " 693: 'shattering',\n",
       " 694: 'douglas',\n",
       " 695: 'able',\n",
       " 696: 'gently',\n",
       " 697: 'ultimate',\n",
       " 698: 'lohman',\n",
       " 699: 'gra',\n",
       " 700: 'affirms',\n",
       " 701: 'deepa',\n",
       " 702: 'pan',\n",
       " 703: 'dogtown',\n",
       " 704: 'plenty',\n",
       " 705: 'could',\n",
       " 706: 'danny',\n",
       " 707: 'williams',\n",
       " 708: 'twist',\n",
       " 709: 'thought',\n",
       " 710: 'hatosy',\n",
       " 711: 'uneven',\n",
       " 712: 'alan',\n",
       " 713: 'fresh',\n",
       " 714: 'toes',\n",
       " 715: 'whereas',\n",
       " 716: 'triple',\n",
       " 717: 'likeable',\n",
       " 718: 'secretary',\n",
       " 719: 'judith',\n",
       " 720: 'photographed',\n",
       " 721: 'confessions',\n",
       " 722: 'simple',\n",
       " 723: 'less',\n",
       " 724: 'notwithstanding',\n",
       " 725: 'functions',\n",
       " 726: 'terry',\n",
       " 727: 'parts',\n",
       " 728: 'reassuring',\n",
       " 729: 'armed',\n",
       " 730: 'enticing',\n",
       " 731: 'going',\n",
       " 732: 'almodovar',\n",
       " 733: 'better',\n",
       " 734: 'tense',\n",
       " 735: 'hard',\n",
       " 736: 'me',\n",
       " 737: 'inherently',\n",
       " 738: 'pure',\n",
       " 739: 'fast',\n",
       " 740: 'devos',\n",
       " 741: 'fulfills',\n",
       " 742: 'wickedly',\n",
       " 743: 'until',\n",
       " 744: 'fincher',\n",
       " 745: 'exploits',\n",
       " 746: 'ratliff',\n",
       " 747: 'passions',\n",
       " 748: 'contrived',\n",
       " 749: 'lavishly',\n",
       " 750: 'combines',\n",
       " 751: 'scooby',\n",
       " 752: 'remove',\n",
       " 753: 'gooding',\n",
       " 754: 'see',\n",
       " 755: 'features',\n",
       " 756: 'bravo',\n",
       " 757: 'lead',\n",
       " 758: 'rife',\n",
       " 759: 'rubbo',\n",
       " 760: 'overcomes',\n",
       " 761: 'touch',\n",
       " 762: 'astonishingly',\n",
       " 763: 'fubar',\n",
       " 764: 'slight',\n",
       " 765: 'unfolds',\n",
       " 766: 'worse',\n",
       " 767: 'canada',\n",
       " 768: 'alternately',\n",
       " 769: 'yes',\n",
       " 770: 'hartley',\n",
       " 771: 'insanely',\n",
       " 772: 'anyone',\n",
       " 773: 'bowling',\n",
       " 774: 'talk',\n",
       " 775: 'white',\n",
       " 776: 'refreshing',\n",
       " 777: 'return',\n",
       " 778: 'suffice',\n",
       " 779: 'marvelously',\n",
       " 780: 'divertida',\n",
       " 781: 'deserves',\n",
       " 782: 'confounding',\n",
       " 783: 'fulford',\n",
       " 784: 'featherweight',\n",
       " 785: 'diggs',\n",
       " 786: 'entirely',\n",
       " 787: 'terrific',\n",
       " 788: 'wonderful',\n",
       " 789: 'ian',\n",
       " 790: 'sports',\n",
       " 791: 'directors',\n",
       " 792: 'labute',\n",
       " 793: 'irwin',\n",
       " 794: 'film',\n",
       " 795: 'contradicts',\n",
       " 796: 'filmmaker',\n",
       " 797: 'rises',\n",
       " 798: 'kids',\n",
       " 799: 'leaping',\n",
       " 800: 'sean',\n",
       " 801: 'she',\n",
       " 802: 'payne',\n",
       " 803: 'try',\n",
       " 804: 'slow',\n",
       " 805: 'austin',\n",
       " 806: 'elvira',\n",
       " 807: 'hashiguchi',\n",
       " 808: 'f',\n",
       " 809: 'shiri',\n",
       " 810: 'jeong',\n",
       " 811: 'grainy',\n",
       " 812: 'daily',\n",
       " 813: 'sex',\n",
       " 814: 'compared',\n",
       " 815: 'delivers',\n",
       " 816: 'vincent',\n",
       " 817: 'fierce',\n",
       " 818: 'cletis',\n",
       " 819: 'campanella',\n",
       " 820: 'either',\n",
       " 821: 'gripping',\n",
       " 822: 'standing',\n",
       " 823: 'rehearsals',\n",
       " 824: 'your',\n",
       " 825: 'bloody',\n",
       " 826: 'faithful',\n",
       " 827: 'intelligent',\n",
       " 828: 'fathers',\n",
       " 829: 'last',\n",
       " 830: 'el',\n",
       " 831: 'can',\n",
       " 832: 'goes',\n",
       " 833: 'unexpected',\n",
       " 834: 'audiard',\n",
       " 835: 'credit',\n",
       " 836: 'witherspoon',\n",
       " 837: 'ambitious',\n",
       " 838: 'eric',\n",
       " 839: 'blithely',\n",
       " 840: 'triumph',\n",
       " 841: 'discursive',\n",
       " 842: 'salma',\n",
       " 843: 'deflated',\n",
       " 844: 'performances',\n",
       " 845: 'undoubtedly',\n",
       " 846: 'personal',\n",
       " 847: 'throughout',\n",
       " 848: 'flat',\n",
       " 849: 'nachtwey',\n",
       " 850: 'robert',\n",
       " 851: 'american',\n",
       " 852: 'fontaine',\n",
       " 853: 'p',\n",
       " 854: 'twenty',\n",
       " 855: 'schepisi',\n",
       " 856: 'lejos',\n",
       " 857: 'interesante',\n",
       " 858: 'got',\n",
       " 859: 'astonishing',\n",
       " 860: 'land',\n",
       " 861: 'blue',\n",
       " 862: 'deliciously',\n",
       " 863: 'suspend',\n",
       " 864: 'trades',\n",
       " 865: 'implicitly',\n",
       " 866: 'victor',\n",
       " 867: 'topics',\n",
       " 868: 'none',\n",
       " 869: 'diane',\n",
       " 870: 'takashi',\n",
       " 871: 'exhilarating',\n",
       " 872: 'because',\n",
       " 873: 'originality',\n",
       " 874: 'walter',\n",
       " 875: 'creeps',\n",
       " 876: 'spider',\n",
       " 877: 'si',\n",
       " 878: 'saddled',\n",
       " 879: 'russell',\n",
       " 880: 'otto',\n",
       " 881: 'demonstrates',\n",
       " 882: 'expect',\n",
       " 883: 'cox',\n",
       " 884: 'halloween',\n",
       " 885: 'romantic',\n",
       " 886: 'bouquet',\n",
       " 887: 'barbershop',\n",
       " 888: 'tully',\n",
       " 889: 'kiarostami',\n",
       " 890: 'murder',\n",
       " 891: 'texan',\n",
       " 892: 'mama',\n",
       " 893: 'seeing',\n",
       " 894: 'speaks',\n",
       " 895: 'hailed',\n",
       " 896: 'be',\n",
       " 897: 'thumbs',\n",
       " 898: 'short',\n",
       " 899: 'kosminsky',\n",
       " 900: 'bound',\n",
       " 901: 'vampire',\n",
       " 902: 'jackie',\n",
       " 903: 'brown',\n",
       " 904: 'assured',\n",
       " 905: 'para',\n",
       " 906: 'neatly',\n",
       " 907: 'frequent',\n",
       " 908: '10',\n",
       " 909: 'cube',\n",
       " 910: 'barry',\n",
       " 911: 'changing',\n",
       " 912: 'borrows',\n",
       " 913: 'puts',\n",
       " 914: 'intimate',\n",
       " 915: 'excellent',\n",
       " 916: 'jolting',\n",
       " 917: 'close',\n",
       " 918: 'skillfully',\n",
       " 919: 'damon',\n",
       " 920: 'beautiful',\n",
       " 921: 'transforms',\n",
       " 922: 'sayles',\n",
       " 923: 'solondz',\n",
       " 924: 'captures',\n",
       " 925: 'imamura',\n",
       " 926: 'exactly',\n",
       " 927: 'rogers',\n",
       " 928: 'deliberately',\n",
       " 929: 'nicholson',\n",
       " 930: 'plummer',\n",
       " 931: 'estupendamente',\n",
       " 932: 'rodriguez',\n",
       " 933: 'amy',\n",
       " 934: 'garcia',\n",
       " 935: 'heartwarming',\n",
       " 936: 'home',\n",
       " 937: 'scott',\n",
       " 938: 'wallace',\n",
       " 939: 'marshall',\n",
       " 940: 'soderbergh',\n",
       " 941: 'hatfield',\n",
       " 942: 'infidelity',\n",
       " 943: 'audacious',\n",
       " 944: 'humorous',\n",
       " 945: 'rouge',\n",
       " 946: 'o',\n",
       " 947: 'feardotcom',\n",
       " 948: 'stuart',\n",
       " 949: 'boasts',\n",
       " 950: 'nakata',\n",
       " 951: 'satin',\n",
       " 952: 'cannon',\n",
       " 953: 'ok',\n",
       " 954: 'craig',\n",
       " 955: 'sorvino',\n",
       " 956: 'great',\n",
       " 957: 'qutting',\n",
       " 958: 'spellbinding',\n",
       " 959: 'impossible',\n",
       " 960: 'many',\n",
       " 961: 'henry',\n",
       " 962: 'playing',\n",
       " 963: 'griffin',\n",
       " 964: 'lacks',\n",
       " 965: 'brims',\n",
       " 966: 'fudges',\n",
       " 967: 'nohe',\n",
       " 968: 'secret',\n",
       " 969: 'weird',\n",
       " 970: 'moving',\n",
       " 971: 'brave',\n",
       " 972: 'sad',\n",
       " 973: 'seeks',\n",
       " 974: 'mordantly',\n",
       " 975: 'earnest',\n",
       " 976: 'vibrantly',\n",
       " 977: 'sweetly',\n",
       " 978: 'bursting',\n",
       " 979: 'kudos',\n",
       " 980: 'colorful',\n",
       " 981: 'westbrook',\n",
       " 982: 'harmless',\n",
       " 983: 'alternates',\n",
       " 984: 'samuel',\n",
       " 985: 'trapped',\n",
       " 986: 'maid',\n",
       " 987: 'daringly',\n",
       " 988: 'hugely',\n",
       " 989: 'swimming',\n",
       " 990: 'blessed',\n",
       " 991: 'turturro',\n",
       " 992: 'meant',\n",
       " 993: 'simply',\n",
       " 994: 'fred',\n",
       " 995: 'each',\n",
       " 996: 'horns',\n",
       " 997: 'oddly',\n",
       " 998: 'captivates',\n",
       " 999: 'how',\n",
       " ...}"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab.idx_to_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewDataset(Dataset):\n",
    "  def __init__(self, X, y, vocab: Vocab):\n",
    "    self.X = X\n",
    "    self.y = y\n",
    "    self.vocab = vocab\n",
    "\n",
    "  def vectorize(self, review):\n",
    "        abc = []\n",
    "        review = nltk.word_tokenize(review)\n",
    "        for i in review:\n",
    "            if i in list(self.vocab.idx_to_token.values()):\n",
    "                get_index = list(self.vocab.idx_to_token.values()).index(i)\n",
    "                abc.append(get_index)\n",
    "        while len(abc)<self.vocab.max_seq_len:\n",
    "            abc.append(0)\n",
    "        return torch.tensor(abc, dtype=torch.int64)\n",
    "\n",
    "\n",
    "  def __len__(self):\n",
    "    return len(self.X)\n",
    "\n",
    "  def __getitem__(self, idx):\n",
    "    return self.vectorize(self.X.iloc[idx]), self.y.iloc[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab(X_train.tolist())\n",
    "RD_train = ReviewDataset(X_train, y_train, vocab)\n",
    "RD_test = ReviewDataset(X_test, y_test, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 13, 155,  42, 269, 269, 870,   3, 365, 629,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0]),\n",
       " 1)"
      ]
     },
     "execution_count": 220,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RD_train[15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_dl = DataLoader(RD_train, batch_size, shuffle=True)\n",
    "test_dl = DataLoader(RD_test, batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   9,    3, 1729,  361,  141, 1497,    3,    3,  370,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "           0,    0])"
      ]
     },
     "execution_count": 222,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = next(iter(train_dl))\n",
    "i[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# подсказал Тимлфей Лашуков и Саша Волненко\n",
    "class Reshaper(nn.Module):\n",
    "\n",
    "  def __init__(self):\n",
    "    super().__init__()\n",
    "\n",
    "  def forward(self, x):\n",
    "    x = torch.permute(x, (0, 2, 1))\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [],
   "source": [
    "class sur_model2(nn.Module):\n",
    "    def __init__(self, input_size, vocab_size, output_size, emb_size):\n",
    "        super(sur_model2, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seqnet = nn.Sequential(nn.Embedding(self.vocab_size, emb_size),\n",
    "            Reshaper(),                       \n",
    "            nn.Conv1d(emb_size, 64, 3),\n",
    "            nn.BatchNorm1d(64),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool1d(2),                        \n",
    "            nn.Conv1d(64, 256, 2),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool1d(2),                        \n",
    "            nn.Conv1d(256, 512, 2),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ELU(),\n",
    "            nn.MaxPool1d(2),\n",
    "            nn.Flatten(), \n",
    "            nn.Linear(3072, 256),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ELU(),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "        self.fc = nn.Linear(64, output_size)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "      out = self.seqnet(x)\n",
    "      out = self.fc(out)\n",
    "      return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = vocab.max_seq_len\n",
    "vocab_size = vocab.vocab_len\n",
    "output_size = 2\n",
    "emb_size = 64\n",
    "model = sur_model2(input_size, vocab_size, output_size, emb_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()   \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_dl, optimizer, total):\n",
    "    #set model in train() mode:\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0.0\n",
    "    total_samples = 0.0\n",
    "    correct_samples = 0.0\n",
    "    \n",
    "    for i, (inputs, targets) in tqdm(enumerate(train_dl), total= total, desc='Training minibatch loop '):\n",
    "        # Forward pass\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward and optimize        \n",
    "        # zero grad before new step        \n",
    "        optimizer.zero_grad()                        \n",
    "        loss.backward()\n",
    "        optimizer.step()   \n",
    "\n",
    "        # calculating the total_loss for checking\n",
    "        total_loss += loss           \n",
    "        \n",
    "        # PREDICTIONS \n",
    "        total_samples += targets.shape[0]   \n",
    "        _, predictions_indices = torch.max(outputs, 1) # dim=1 - dimension to reduce\n",
    "        correct_samples += torch.sum(predictions_indices==targets)\n",
    "\n",
    "    train_accuracy = float(correct_samples) / total_samples        \n",
    "    \n",
    "    return total_loss, train_accuracy                        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TESTING THE MODEL\n",
    "def test(model, device, test_dl, total):\n",
    "    #set model in eval() mode (it skips Dropout etc):\n",
    "    model.eval()\n",
    "    \n",
    "    total_samples = 0.0\n",
    "    correct_samples = 0.0 \n",
    "    \n",
    "    # set the requires_grad flag to false as we are in the test mode\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, targets) in tqdm(enumerate(test_dl), total=total, desc='Testing minibatch loop:'):                      \n",
    "            # apply model to input data\n",
    "            outputs = model(inputs)        \n",
    "                       \n",
    "            #PREDICTIONS\n",
    "            total_samples += targets.shape[0]   \n",
    "            _, predictions_indices = torch.max(outputs, 1) # dim=1 - dimension to reduce\n",
    "            correct_samples += torch.sum(predictions_indices==targets)                    \n",
    "        \n",
    "    test_accuracy = correct_samples / total_samples        \n",
    "    \n",
    "    return test_accuracy              "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "01eee2af66924a1697df097c9a195b9a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02afc1cec7474e28baa1136707bae803",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/25], Loss: 57.4314, Train acc: 0.9000, Test acc: 0.6005\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f120b428ff24279bcd2ddbe613fb5f6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d4181b304e4e88b4482c4a7390c38d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/25], Loss: 44.3974, Train acc: 0.9254, Test acc: 0.6518\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6265d99b4a1b46da8db8a88535828ba7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cca31f9f0a1c473a84352818fa666e75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3/25], Loss: 34.6210, Train acc: 0.9382, Test acc: 0.6496\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "971655e0f7184f6ab5bb0ce506feeb5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0b7cc06667ab47249ff395a32ae532eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4/25], Loss: 28.1670, Train acc: 0.9514, Test acc: 0.6280\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "576fd48bdf8d41e0829efbf8d560f804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f294535faf45ec8b8b2dd9826eda3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5/25], Loss: 25.7167, Train acc: 0.9585, Test acc: 0.6461\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14002bd3696e4032bdc662f794a48356",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b50b4be8fd4450a869f2dac4f4dcf9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/25], Loss: 22.3572, Train acc: 0.9629, Test acc: 0.6374\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa321b62db18494c9ebfbd007c8ce7da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39014f5028874582aa9d6cb165858a5c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7/25], Loss: 19.4793, Train acc: 0.9692, Test acc: 0.6236\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "702f755ad7ae4c008a743ee5d2bd35fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b9d30a690004feca0f697fc9c8319df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/25], Loss: 18.9018, Train acc: 0.9693, Test acc: 0.6111\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4d576c7f4874c569a99574f40f6855d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb4a2ffe0f4a4049b0bf3f30d7ee724d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9/25], Loss: 16.4961, Train acc: 0.9724, Test acc: 0.6268\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1ec4ec754224b368a60c09a93189d6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "434185832c0f444d8f0b53a62d995efb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/25], Loss: 15.7942, Train acc: 0.9737, Test acc: 0.6405\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dc5f96630664de2b9a13e0edc6d2cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5f16d60676b48778b186a243884ebc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [11/25], Loss: 14.6339, Train acc: 0.9748, Test acc: 0.6380\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb3e8c32f9d84bc18289c41b18ccf2a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f76197ddba45e58258f9e2337edbb2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [12/25], Loss: 12.4255, Train acc: 0.9798, Test acc: 0.6424\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1201691c30624f35b8e64b6e0ea8e0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc76d48e6a6949b2a66bd33b31261fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [13/25], Loss: 11.9332, Train acc: 0.9802, Test acc: 0.5896\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "361bf68feb914052b2393751cfb657b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d754c015813d48919ed1098f5b9c5e04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [14/25], Loss: 12.9689, Train acc: 0.9780, Test acc: 0.6358\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "400ad7aa088d48d7ace66e0196dd1639",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea81488e3e248709a58b6f56d2cd94a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [15/25], Loss: 9.7589, Train acc: 0.9823, Test acc: 0.6415\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ff6b65b59f44ac5bfc3f64379a4a671",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5ed5a4708b5a477ca46425244f849aa4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/25], Loss: 9.1722, Train acc: 0.9816, Test acc: 0.6333\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8977805924e5420da6d423e39a90bbf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b60f9f3513649eea5811d87aefeb5b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [17/25], Loss: 10.2986, Train acc: 0.9814, Test acc: 0.5986\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ca16419efdd4594a330e60181ef1136",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13d1eefdabdd4d898dc0fcf8e116e1b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [18/25], Loss: 10.4075, Train acc: 0.9827, Test acc: 0.6474\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1fc9437f30c84f61a723f2b4d40771d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2ca9f134ef4149258eb8b92356b9345c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [19/25], Loss: 9.9366, Train acc: 0.9833, Test acc: 0.6280\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55d612001e9d4a99b6cd5357a52fa5b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c7508a05a8b4110a77b7ede324e5b1b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [20/25], Loss: 10.1633, Train acc: 0.9827, Test acc: 0.6499\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2fe0efe513534d5aa062aa850e0a0d96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09ad0affea9e426ca24752199e3310f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [21/25], Loss: 6.5794, Train acc: 0.9886, Test acc: 0.6374\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a7e589da3da74d9d90117109bbf30d9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0707b4496a44858b3d5f4882bb5ab17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [22/25], Loss: 8.1679, Train acc: 0.9842, Test acc: 0.6458\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3d1e7675804212a18d0241e0b0869d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "243cc614ed0849f9bec3903357d3976e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [23/25], Loss: 8.4113, Train acc: 0.9854, Test acc: 0.6155\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a31fdd59b63a44ab979760737090a064",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c910479b2a7b4e418a02a8cb6f71591a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [24/25], Loss: 9.3008, Train acc: 0.9831, Test acc: 0.6458\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9802d7a0ef894320b5824f38388b76f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training minibatch loop :   0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b9ee7ecc34ee4600a3521825c9fb9826",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Testing minibatch loop::   0%|          | 0/43 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [25/25], Loss: 8.6631, Train acc: 0.9865, Test acc: 0.6333\n"
     ]
    }
   ],
   "source": [
    "## %%time\n",
    "for_losses = []\n",
    "n_total_steps = len(train_iter)\n",
    "\n",
    "\n",
    "for epoch in range(25):\n",
    "    total_loss, train_accuracy = train(model, device, train_dl, optimizer, total = math.ceil(len(train_ds)/64))\n",
    "    test_accuracy = test(model, device, test_dl, total = math.ceil(len(test_ds)/64))\n",
    "    for_losses.append(total_loss.cpu().detach().numpy())\n",
    "    print (f'Epoch [{epoch+1}/{25}], Loss: {total_loss:.4f}, Train acc: {train_accuracy:.4f}, Test acc: {test_accuracy:.4f}')                                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
